{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahrad/Covid/blob/main/Corona_Taxonomy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzkSry5x4VaL"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLa737hB7e4_"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTrJ7gQ5cG4F",
        "outputId": "ec78bbcd-7a88-4b15-97ef-07c31bd42c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiNZfPTu0_em"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "FILELOC = \"/content/drive/My Drive/COVID_Python/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INmsA86dCdMV"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    tpu_env=True\n",
        "except ValueError:\n",
        "    print('Not connected to a TPU runtime.')\n",
        "    tpu_env=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EfE13Hp8AJ9"
      },
      "outputs": [],
      "source": [
        "def reset_model(regress, singleclass, multiclass, output_multiheadatt, use_att, nclasses=4,\n",
        "                output_two=False):\n",
        "\n",
        "    if output_multiheadatt:\n",
        "        model_fn = AttMod_2\n",
        "    elif output_two:\n",
        "        model_fn = AttMod_3\n",
        "    else:\n",
        "        model_fn = AttModel\n",
        "\n",
        "    model = model_fn(L=ismlen,\n",
        "                     vocab_size=len(aa_list)+1,\n",
        "                     embdim = ENCDIM,\n",
        "                     numheads = NHEADS,\n",
        "                     ffdim = FFDIM,\n",
        "                     num_dense = NDENSE,\n",
        "                     mask_zero=True,\n",
        "                     dropout_rate = DROPRATE,\n",
        "                     trans_drop = TRANSDROPRATE,\n",
        "                     Nt = NT,\n",
        "                     W = 1, Nc = NC, Nl = NL,\n",
        "                     regress=regress, singleclass=singleclass,\n",
        "                     multiclass=multiclass, use_att=use_att,\n",
        "                     nclasses=nclasses,\n",
        "                     )\n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARN_RATE)\n",
        "    if regress:\n",
        "        loss = keras.losses.MeanSquaredError()\n",
        "        metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "            keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "            keras.losses.MeanAbsoluteError(name='mae')\n",
        "            ]\n",
        "    if singleclass:\n",
        "        loss = keras.losses.BinaryCrossentropy()\n",
        "        metrics = [keras.metrics.BinaryAccuracy(name='acc'),\n",
        "                   keras.metrics.AUC(name='auc')]\n",
        "    if multiclass:    \n",
        "        loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "        metrics = [keras.metrics.SparseCategoricalAccuracy(name='acc')]\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics,)\n",
        "                #   steps_per_execution = STEPS_PER_EXECUTION,)\n",
        "\n",
        "    if output_two:\n",
        "        losses = {'outfirst':'mean_squared_error',\n",
        "                  'outpeak':'mean_squared_error'}\n",
        "        lossweights = {'outfirst':1.0, 'outpeak':1.0}\n",
        "        metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "                   keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "                   keras.losses.MeanAbsoluteError(name='mae')]\n",
        "        model.compile(loss=losses, loss_weights=lossweights, optimizer=optimizer,metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGYlfhd95NKk"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, mask_zero=False):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                output_dim=embed_dim,\n",
        "                                                mask_zero=mask_zero)\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,\n",
        "                                              mask_zero=mask_zero)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "def linear01(x):\n",
        "    return tf.clip_by_value(x, clip_value_min=0, clip_value_max=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPCCClpo7Rim"
      },
      "outputs": [],
      "source": [
        "def AttMod_2(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4):\n",
        "\n",
        "    inpTensor = keras.Input(shape=(L,))\n",
        "    x = inpTensor\n",
        "\n",
        "    if mask_zero:\n",
        "        x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    y, attout = keras.layers.MultiHeadAttention(num_heads=numheads, key_dim=embdim,\n",
        "                                                )(x, x, return_attention_scores=True)\n",
        "    y = keras.layers.Dropout(trans_drop)(y)\n",
        "    z = keras.layers.LayerNormalization(epsilon=1e-6)(x + y)\n",
        "    z1 = keras.Sequential( [keras.layers.Dense(ffdim, activation=\"relu\"), keras.layers.Dense(embdim),])\n",
        "    z1 = keras.layers.Dropout(trans_drop)(z)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(z + z1)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(x)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    model = keras.Model(inpTensor,finalOut)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK05TjTo79ql"
      },
      "outputs": [],
      "source": [
        "def AttModel(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4):\n",
        "\n",
        "    inpTensor = keras.Input(shape=(L,))\n",
        "    x = inpTensor\n",
        "\n",
        "    if mask_zero:\n",
        "        x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(x)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    model = keras.Model(inpTensor,finalOut)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX-ogBerSt8P"
      },
      "outputs": [],
      "source": [
        "def AttMod_3(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4):\n",
        "\n",
        "    inpTensor = keras.Input(shape=(L,))\n",
        "    x = inpTensor\n",
        "\n",
        "    if mask_zero:\n",
        "        x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(x)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    out1 = keras.layers.Dense(1, activation='sigmoid', name='outfirst')(x)\n",
        "    out2 = keras.layers.Dense(1, activation='sigmoid', name='outpeak')(x)\n",
        "    # define the model's start and end points    \n",
        "    model = keras.Model(inpTensor,[out1,out2])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsAfrYxP75be"
      },
      "outputs": [],
      "source": [
        "# These parameters are currently hard-coded\n",
        "ENCDIM = 1500\n",
        "NC = 300\n",
        "NL = 1\n",
        "NT = 1\n",
        "NHEADS = 8\n",
        "FFDIM = 64\n",
        "NDENSE = 64\n",
        "TRANSDROPRATE = 0.1\n",
        "DROPRATE = 0.0\n",
        "\n",
        "LEARN_RATE = 0.0001\n",
        "\n",
        "BATCH_SIZE = 48\n",
        "\n",
        "STEPS_PER_EXECUTION = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiRleV1h3QHw"
      },
      "source": [
        "#Sequence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqXh0Ei76tXD"
      },
      "outputs": [],
      "source": [
        "ismlen = 1500\n",
        "\n",
        "def tokenize_sequences(data_dataframe, SeqCol='ISM', seqlen=1273):\n",
        "    def f(x):\n",
        "        if len(x) < seqlen:\n",
        "            return x + '*'*(seqlen-len(x))\n",
        "        elif len(x) > seqlen:\n",
        "            return x[:seqlen]\n",
        "        else:\n",
        "            return x\n",
        "    data = np.vstack(data_dataframe[SeqCol].apply(f).apply(lambda x: np.array(list(x))))\n",
        "    aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "            'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "            'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "            ]\n",
        "    aa_tokenizer = {aa_list[k]:k+1 for k in range(len(aa_list))}\n",
        "    aa_tokenizer['*'] = 0\n",
        "    aa_tokenizer['X'] = 0\n",
        "    # optionally handle B, J, Z ambiguities\n",
        "    # Asx\tB\tAspartic acid or Asparagine (D or N)\n",
        "    # Glx\tZ\tGlutamic acid or Glutamine (E or Q)\n",
        "    # Xaa\tX\tAny amino acid\n",
        "    # Xle\tJ\tLeucine or Isoleucine (L or I)\n",
        "    aa_tokenizer['B'] = 0\n",
        "    aa_tokenizer['Z'] = 0\n",
        "    aa_tokenizer['J'] = 0\n",
        "\n",
        "    return np.vectorize(aa_tokenizer.get)(data)\n",
        "\n",
        "aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "        'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "        'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jB3nt5uf0EB"
      },
      "outputs": [],
      "source": [
        "# datadf = pd.read_csv(FILELOC + 'species_dataset_reduced_20211121.csv')\n",
        "\n",
        "# datadf = pd.read_csv(FILELOC + 'species_dataset_reduced_20211127.csv')\n",
        "# print(len(datadf))\n",
        "# datadf.drop_duplicates('Seq', inplace=True)\n",
        "# datadf.reset_index(drop=False, inplace=True)\n",
        "# datadf.rename(columns={'index':'original_index'}, inplace=True)\n",
        "# print(len(datadf))\n",
        "# datadf.to_csv(FILELOC + 'species_dataset_reduced_20211127_dropduplicates.csv', index=False)\n",
        "\n",
        "# datadf = pd.read_csv(FILELOC + 'species_dataset_reduced_20211127_dropduplicates.csv')\n",
        "\n",
        "# datadf.to_excel('Supplemental Table 2.xls', index=False)\n",
        "\n",
        "# REMOVE SEQUENCES THAT WERE FOUND AFTER COVID EMERGED (i.e. near neighbors to SARS-Cov-2)\n",
        "\n",
        "# from dateutil.parser import parse as dateparse\n",
        "# datadf['date_parsed'] = datadf.date.apply(dateparse)\n",
        "# print(len(datadf))\n",
        "# datadf.drop(datadf[(datadf.Species.str.contains('Pangolin')) & (datadf.date_parsed > dateparse('2020-01-01'))].index,\n",
        "#             inplace=True)\n",
        "# print(len(datadf))\n",
        "# datadf.to_csv(FILELOC + 'species_dataset_20220112.csv')\n",
        "\n",
        "datadf = pd.read_csv(FILELOC + 'species_dataset_20220112.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPSk4dLObrZM"
      },
      "outputs": [],
      "source": [
        "datadf['seqlen'] = datadf.Seq.apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfM-NmFNf8He"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(datadf, 'Seq', 1500)\n",
        "y = datadf.genuslabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIjNQzFbcBN2"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(datadf[datadf.seqlen>1000], 'Seq', 1500)\n",
        "y = datadf[datadf.seqlen>1000].genuslabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnHgQJXxgOPg"
      },
      "outputs": [],
      "source": [
        "# CODE TO GENERATE RANDOM TRAIN/TEST SPLITS\n",
        "# saves the identity of those splits for each of the possible input data sets\n",
        "\n",
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.9*len(seqtok)), replace=False)\n",
        "# testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "# np.savetxt(FILELOC + 'species_reduced_trainindex_2021121.csv', trainindex, fmt='%i', delimiter=',')\n",
        "# trainindex = np.loadtxt(FILELOC + 'species_reduced_trainindex_2021121.csv', dtype=int, delimiter=',')\n",
        "# testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "# xtraintok = seqtok[trainindex]; ytrain = y[trainindex]\n",
        "# xtesttok = seqtok[testindex]; ytest = y[testindex]\n",
        "\n",
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.9*len(seqtok)), replace=False)\n",
        "# testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "# np.savetxt(FILELOC + 'species_reduced_trainindex_20211127_dropduplicates.csv', trainindex, fmt='%i', delimiter=',')\n",
        "# trainindex = np.loadtxt(FILELOC + 'species_reduced_trainindex_20211127.csv', dtype=int, delimiter=',')\n",
        "# trainindex = np.loadtxt(FILELOC + 'species_reduced_trainindex_20211127_dropduplicates.csv', dtype=int, delimiter=',')\n",
        "# testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "# xtraintok = seqtok[trainindex]; ytrain = y[trainindex]\n",
        "# xtesttok = seqtok[testindex]; ytest = y[testindex]\n",
        "\n",
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.9*len(seqtok)), replace=False)\n",
        "# testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "# np.savetxt(FILELOC + 'species_trainindex_20220112.csv', trainindex, fmt='%i', delimiter=',')\n",
        "# trainindex = np.loadtxt(FILELOC + 'species_trainindex_20220112.csv', dtype=int, delimiter=',')\n",
        "# trainindex = np.loadtxt(FILELOC + 'species_trainindex_20220112.csv', dtype=int, delimiter=',')\n",
        "# testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "# xtraintok = seqtok[trainindex]; ytrain = y[trainindex]\n",
        "# xtesttok = seqtok[testindex]; ytest = y[testindex]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Class Balancing"
      ],
      "metadata": {
        "id": "ihIuPitbz8eh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2D1JqUZgsxf",
        "outputId": "3fa3f490-6820-4c60-a24e-f2ebf81386ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.1203703703703705, 0.6364894795127354, 0.8668929110105581, 2.6125]\n"
          ]
        }
      ],
      "source": [
        "# from sklearn.utils import class_weight\n",
        "# class_weights = list(class_weight.compute_class_weight(class_weight='balanced',\n",
        "#                                                        classes=np.unique(ytrain), y=ytrain))\n",
        "# sample_weights = np.array([class_weights[int(y)] for y in ytrain])\n",
        "# print(class_weights)\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = list(class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                       classes=np.unique(y), y=y))\n",
        "sample_weights = np.array([class_weights[int(yi)] for yi in y])\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "2g0XJmQDz-Cj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qNvsD6U7Ej5",
        "outputId": "61ff2678-7d5c-40d8-baf1-55728f15da05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "48/48 [==============================] - 19s 95ms/step - loss: 1.0997 - acc: 0.5698\n",
            "Epoch 2/20\n",
            "48/48 [==============================] - 5s 94ms/step - loss: 0.5616 - acc: 0.8799\n",
            "Epoch 3/20\n",
            "48/48 [==============================] - 5s 94ms/step - loss: 0.2830 - acc: 0.9526\n",
            "Epoch 4/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.1737 - acc: 0.9696\n",
            "Epoch 5/20\n",
            "48/48 [==============================] - 5s 94ms/step - loss: 0.1214 - acc: 0.9752\n",
            "Epoch 6/20\n",
            "48/48 [==============================] - 5s 94ms/step - loss: 0.0878 - acc: 0.9826\n",
            "Epoch 7/20\n",
            "48/48 [==============================] - 5s 94ms/step - loss: 0.0692 - acc: 0.9878\n",
            "Epoch 8/20\n",
            "48/48 [==============================] - 5s 95ms/step - loss: 0.0618 - acc: 0.9891\n",
            "Epoch 9/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0469 - acc: 0.9922\n",
            "Epoch 10/20\n",
            "48/48 [==============================] - 4s 94ms/step - loss: 0.0357 - acc: 0.9939\n",
            "Epoch 11/20\n",
            "48/48 [==============================] - 5s 94ms/step - loss: 0.0280 - acc: 0.9957\n",
            "Epoch 12/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0221 - acc: 0.9957\n",
            "Epoch 13/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0185 - acc: 0.9978\n",
            "Epoch 14/20\n",
            "48/48 [==============================] - 4s 94ms/step - loss: 0.0132 - acc: 0.9987\n",
            "Epoch 15/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0148 - acc: 0.9987\n",
            "Epoch 16/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0107 - acc: 0.9991\n",
            "Epoch 17/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0068 - acc: 1.0000\n",
            "Epoch 18/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 19/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 20/20\n",
            "48/48 [==============================] - 4s 93ms/step - loss: 0.0057 - acc: 1.0000\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 48\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',\n",
        "    verbose = 1,\n",
        "    patience = 10, #5,\n",
        "    mode = 'auto',\n",
        "    min_delta = 0,\n",
        "    restore_best_weights = True\n",
        "    )\n",
        "\n",
        "for run in range(1,2):\n",
        "    tf.keras.backend.clear_session()\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                            output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    history = model.fit(seqtok, y,\n",
        "                        # xtraintok, ytrain,\n",
        "                        sample_weight = sample_weights,\n",
        "                        batch_size = BATCH_SIZE,\n",
        "                        epochs = NUM_EPOCHS,\n",
        "                        verbose = 1,\n",
        "                        # validation_split = VAL_SPLIT,\n",
        "                        # callbacks = [early_stopping],\n",
        "                        )\n",
        "    # model.save_weights(f\"{FILELOC}taxonomy_weights_20220112_seq1000_{run}.h5\", save_format='h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model"
      ],
      "metadata": {
        "id": "3QHL5X_40BNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw67JF2dkk9B"
      },
      "outputs": [],
      "source": [
        "# model.save_weights(FILELOC + 'coronavirus_spike_taxonomy_20211127_dropduplicates.h5', save_format='h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show multilabel classification matrix"
      ],
      "metadata": {
        "id": "7zwuIWcA0CxI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYZ5QprR7JlK"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import multilabel_confusion_matrix\n",
        "# from collections import Counter\n",
        "# for run in range(1,6):\n",
        "#     tf.keras.backend.clear_session()\n",
        "#     with tpu_strategy.scope():\n",
        "#         model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "#                         output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "#         model.load_weights(f\"{FILELOC}taxonomy_weights_20211213_{run}.h5\")\n",
        "#         model.compile()\n",
        "#     print(f'Run {run}')\n",
        "#     print(multilabel_confusion_matrix(ytest, model.predict(xtest).argmax(axis=1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFUxcE3kTLrm"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K1XtrNTLDDZ"
      },
      "source": [
        "##Load Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySoJYkQcLJB7"
      },
      "outputs": [],
      "source": [
        "ismlen = 1500\n",
        "\n",
        "def tokenize_sequences(data_dataframe, SeqCol='ISM', seqlen=1273):\n",
        "    def f(x):\n",
        "        if len(x) < seqlen:\n",
        "            return x + '*'*(seqlen-len(x))\n",
        "        elif len(x) > seqlen:\n",
        "            return x[:seqlen]\n",
        "        else:\n",
        "            return x\n",
        "    data = np.vstack(data_dataframe[SeqCol].apply(f).apply(lambda x: np.array(list(x))))\n",
        "    aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "            'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "            'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "            ]\n",
        "    aa_tokenizer = {aa_list[k]:k+1 for k in range(len(aa_list))}\n",
        "    aa_tokenizer['*'] = 0\n",
        "    aa_tokenizer['X'] = 0\n",
        "    # optionally handle B, J, Z ambiguities\n",
        "    # Asx\tB\tAspartic acid or Asparagine (D or N)\n",
        "    # Glx\tZ\tGlutamic acid or Glutamine (E or Q)\n",
        "    # Xaa\tX\tAny amino acid\n",
        "    # Xle\tJ\tLeucine or Isoleucine (L or I)\n",
        "    aa_tokenizer['B'] = 0\n",
        "    aa_tokenizer['Z'] = 0\n",
        "    aa_tokenizer['J'] = 0\n",
        "\n",
        "    return np.vectorize(aa_tokenizer.get)(data)\n",
        "\n",
        "aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "        'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "        'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2oB88Hchbzb"
      },
      "source": [
        "Example of generating predictions, embeddings, and attention values for a pretrained model (using full species dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXjnNBtE8f8S"
      },
      "outputs": [],
      "source": [
        "sdf = pd.read_csv(FILELOC + \"species_dataset_20211127.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0eVRRo8hKb"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "with tpu_strategy.scope():\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    model.load_weights(f\"{FILELOC}taxonomy_weights_pretrained.h5\")\n",
        "    model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRJWe8_a8jBe",
        "outputId": "0b641cf2-a1fb-481f-c006-0a1844652e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2975: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2975: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        }
      ],
      "source": [
        "tok = tokenize_sequences(sdf,'Seq',1500)\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()\n",
        "    pred = model.predict(tok)\n",
        "    emb = get_embedding_model.predict(tok)\n",
        "    att = get_attention_model.predict(tok)\n",
        "    sdf['pred'] = [p for p in pred]\n",
        "    sdf['emb'] = [e for e in emb]\n",
        "    sdf['att'] = [a for a in att]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Corona_Taxonomy.ipynb",
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1nxzD1atzba5pxRhbeCs0gNB0spnhUWJh",
      "authorship_tag": "ABX9TyO8JknNwcf9geJZjkXg6Ueu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}