{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahrad/Covid/blob/main/Corona_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzkSry5x4VaL"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dependencies"
      ],
      "metadata": {
        "id": "GfXj-B_9RmZv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLa737hB7e4_"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.parser import parse as dateparse\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, QuantileTransformer, OneHotEncoder, LabelBinarizer, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RandomizedSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score,classification_report, make_scorer, balanced_accuracy_score, coverage_error, roc_auc_score, confusion_matrix, plot_confusion_matrix, multilabel_confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.utils import class_weight\n",
        "import sklearn as sk\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import make_pipeline,Pipeline\n",
        "\n",
        "# !python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If using Google colab, mount drive and set location"
      ],
      "metadata": {
        "id": "a8AvDkm_RpF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiNZfPTu0_em"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "FILELOC = \"/content/drive/My Drive/COVID_Python/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activate TPU resources if available"
      ],
      "metadata": {
        "id": "uOZ1irNHRrCv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INmsA86dCdMV"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    tpu_env=True\n",
        "except ValueError:\n",
        "    print('Not connected to a TPU runtime.')\n",
        "    tpu_env=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPgZv7LUzHDV"
      },
      "source": [
        "##Initialize Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to define the model.\n",
        "\n",
        "* \"regress\" sets the model to output a continuous value from 0 to 1 for regression.\n",
        "* \"singleclass\" is for a 0/1 binary output.\n",
        "* \"multiclass\" outputs a softmax for a number of classes. The code may be readily modified for multilabel classification as well.\n",
        "* \"output_multiheadatt\" generates the attention values for the transformer model, which may be extracted later.\n",
        "* \"use_att\" adds a flat sequence-wide attention layer.\n",
        "* \"output_two\" allows for two outputs, which maybe jointly used to compute the loss and optimize the model.\n",
        "* \"nclasses\" is the number of classes (ignored for regress or singleclass modes\n",
        "* \"mask\" allows for masking zeros in the sequence. \"numvars\" \n",
        "* \"numvars\" if set to a non-zero value adds additional variables as input (such as age/date/gender for clinical severity calculation) which are concatenated after the transformer and/or sequence-wide flat attention layer (if \"use_att\" is set to True) "
      ],
      "metadata": {
        "id": "MURAx4QiRtF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EfE13Hp8AJ9"
      },
      "outputs": [],
      "source": [
        "def reset_model(regress, singleclass, multiclass, output_multiheadatt, use_att, nclasses=4,\n",
        "                output_two=False, mask=True, numvars = 0):\n",
        "\n",
        "    if output_multiheadatt:\n",
        "        model_fn = AttMod_2\n",
        "    elif output_two:\n",
        "        model_fn = AttMod_3\n",
        "    else:\n",
        "        model_fn = AttModel\n",
        "\n",
        "    model = model_fn(L=SEQLEN,\n",
        "                     vocab_size=len(aa_list)+1,\n",
        "                     embdim = ENCDIM,\n",
        "                     numheads = NHEADS,\n",
        "                     ffdim = FFDIM,\n",
        "                     num_dense = NDENSE,\n",
        "                     mask_zero = mask,\n",
        "                     dropout_rate = DROPRATE,\n",
        "                     trans_drop = TRANSDROPRATE,\n",
        "                     Nt = NT,\n",
        "                     W = 1, Nc = NC, Nl = NL,\n",
        "                     regress=regress,\n",
        "                     singleclass=singleclass,\n",
        "                     multiclass=multiclass,\n",
        "                     use_att=use_att,\n",
        "                     nclasses=nclasses,\n",
        "                     nvars=numvars\n",
        "                     )\n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARN_RATE)\n",
        "    if regress:\n",
        "        loss = keras.losses.MeanSquaredError()\n",
        "        metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "            keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "            keras.losses.MeanAbsoluteError(name='mae')\n",
        "            ]\n",
        "    if singleclass:\n",
        "        loss = keras.losses.BinaryCrossentropy()\n",
        "        metrics = [keras.metrics.BinaryAccuracy(name='acc'),\n",
        "                   keras.metrics.AUC(name='auc')]\n",
        "    if multiclass:    \n",
        "        loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "        metrics = [keras.metrics.SparseCategoricalAccuracy(name='acc')]\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics,)\n",
        "                #   steps_per_execution = STEPS_PER_EXECUTION,)\n",
        "\n",
        "    if output_two:\n",
        "        losses = {'outfirst':'mean_squared_error',\n",
        "                  'outpeak':'mean_squared_error'}\n",
        "        lossweights = {'outfirst':1.0, 'outpeak':1.0}\n",
        "        metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "                   keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "                   keras.losses.MeanAbsoluteError(name='mae')]\n",
        "        model.compile(loss=losses, loss_weights=lossweights, optimizer=optimizer,metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Parameters"
      ],
      "metadata": {
        "id": "jGGOMX8bTDWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzpJhreYnR70"
      },
      "outputs": [],
      "source": [
        "# These parameters are currently hard-coded\n",
        "ENCDIM = 1500\n",
        "NC = 300\n",
        "NL = 1                  # set to 0 to remove CNN pre-filtering\n",
        "NT = 1\n",
        "NHEADS = 8\n",
        "FFDIM = 64\n",
        "NDENSE = 64             # set to 0 to deactivate embedding layer\n",
        "TRANSDROPRATE = 0.4\n",
        "DROPRATE = 0.0\n",
        "\n",
        "LEARN_RATE = 0.0001\n",
        "\n",
        "BATCH_SIZE = 48\n",
        "\n",
        "STEPS_PER_EXECUTION = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOV554GHzJOd"
      },
      "source": [
        "##Model Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer and Token & Position Embedding definitions. Adapted from https://keras.io/examples/nlp/text_classification_with_transformer/"
      ],
      "metadata": {
        "id": "J7aN_3QhTFek"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZrR1_I3KahE"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        attn_output = self.att(inputs, inputs, attention_mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, mask_zero=False):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                output_dim=embed_dim,\n",
        "                                                mask_zero=mask_zero)\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,\n",
        "                                              mask_zero=mask_zero)\n",
        "        self.mask_zero = mask_zero\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "\n",
        "        if self.mask_zero:\n",
        "            mask = x._keras_mask\n",
        "            return x + positions, mask\n",
        "        else:\n",
        "            return x + positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0U-6K6EzO71"
      },
      "source": [
        "###Default model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "ORfdVECNCICt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK05TjTo79ql"
      },
      "outputs": [],
      "source": [
        "def AttModel(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4, nvars=0):\n",
        "\n",
        "    inpSeq = keras.Input(shape=(L,))\n",
        "    if nvars > 0:\n",
        "        # additional variables besides sequence\n",
        "        inpVars = keras.Input(shape=(3,))\n",
        "    x = inpSeq\n",
        "    if nvars > 0:\n",
        "        v = inpVars\n",
        "\n",
        "    # if mask_zero:\n",
        "    #     x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    inpSeq = keras.Input(shape=(L,))\n",
        "    inpVars = keras.Input(shape=(3,))\n",
        "    x = inpSeq\n",
        "    v = inpVars\n",
        "\n",
        "    if mask_zero:\n",
        "        x, mask = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "    else:\n",
        "        x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x, mask_zero)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if nvars > 0:\n",
        "        # concatenate additional variables with the transformer output\n",
        "        h = keras.layers.concatenate([x, v])\n",
        "    else:\n",
        "        h = x\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(h)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    # model = keras.Model(inpTensor, finalOut)\n",
        "    if nvars > 0:\n",
        "        model = keras.Model([inpSeq,inpVars], finalOut)\n",
        "    else:\n",
        "        model = keras.Model(inpSeq, finalOut)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SirUALhzMiF"
      },
      "source": [
        "###Return multihead attention scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if \"output_multiheadatt\" set to True"
      ],
      "metadata": {
        "id": "ZfWJYsvTTSir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPCCClpo7Rim"
      },
      "outputs": [],
      "source": [
        "def AttMod_2(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4, nvars=0):\n",
        "\n",
        "    inpSeq = keras.Input(shape=(L,))\n",
        "    if nvars > 0:\n",
        "        # additional variables besides sequence\n",
        "        inpVars = keras.Input(shape=(3,))\n",
        "    x = inpSeq\n",
        "    if nvars > 0:\n",
        "        v = inpVars\n",
        "\n",
        "    # if mask_zero:\n",
        "    #     x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    if mask_zero:\n",
        "        x, mask = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "    else:\n",
        "        x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    y, attout = keras.layers.MultiHeadAttention(num_heads=numheads, key_dim=Nc,\n",
        "                                                )(x, x, return_attention_scores=True,\n",
        "                                                  attention_mask=mask_zero)\n",
        "    y = keras.layers.Dropout(trans_drop)(y)\n",
        "    z = keras.layers.LayerNormalization(epsilon=1e-6)(x + y)\n",
        "    z1 = keras.Sequential( [keras.layers.Dense(ffdim, activation=\"relu\"), keras.layers.Dense(embdim),])\n",
        "    z1 = keras.layers.Dropout(trans_drop)(z)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(z + z1)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if nvars > 0:\n",
        "        # concatenate additional variables with the transformer output\n",
        "        h = keras.layers.concatenate([x, v])\n",
        "    else:\n",
        "        h = x\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(h)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    # model = keras.Model(inpTensor, finalOut)\n",
        "    if nvars > 0:\n",
        "        model = keras.Model([inpSeq,inpVars], finalOut)\n",
        "    else:\n",
        "        model = keras.Model(inpSeq, finalOut)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P82fJycFzTnF"
      },
      "source": [
        "###Output two predictions for joint optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if \"output_two\" is set to True"
      ],
      "metadata": {
        "id": "W1YRmGhyTRR_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX-ogBerSt8P"
      },
      "outputs": [],
      "source": [
        "def AttMod_3(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4, nvars=0):\n",
        "\n",
        "    inpSeq = keras.Input(shape=(L,))\n",
        "    if nvars > 0:\n",
        "        # additional variables besides sequence\n",
        "        inpVars = keras.Input(shape=(3,))\n",
        "    x = inpSeq\n",
        "    if nvars > 0:\n",
        "        v = inpVars\n",
        "\n",
        "    # if mask_zero:\n",
        "    #     x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    if mask_zero:\n",
        "        x, mask = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "    else:\n",
        "        x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x, mask_zero)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if nvars > 0:\n",
        "        # concatenate additional variables with the transformer output\n",
        "        h = keras.layers.concatenate([x, v])\n",
        "    else:\n",
        "        h = x\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(h)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    out1 = keras.layers.Dense(1, activation='sigmoid', name='outfirst')(x)\n",
        "    out2 = keras.layers.Dense(1, activation='sigmoid', name='outpeak')(x)\n",
        "    # define the model's start and end points    \n",
        "    if nvars > 0:\n",
        "        model = keras.Model([inpSeq,inpVars], [out1,out2])\n",
        "    else:\n",
        "        model = keras.Model(inpSeq, [out1,out2])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcEFJHlTFMag"
      },
      "source": [
        "##Function to tokenize sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-q4xrJCFMam"
      },
      "outputs": [],
      "source": [
        "def tokenize_sequences(data_dataframe, SeqCol='ISM', seqlen=1273):\n",
        "    def f(x):\n",
        "        if len(x) < seqlen:\n",
        "            return x + '*'*(seqlen-len(x))\n",
        "        elif len(x) > seqlen:\n",
        "            return x[:seqlen]\n",
        "        else:\n",
        "            return x\n",
        "    data = np.vstack(data_dataframe[SeqCol].apply(f).apply(lambda x: np.array(list(x))))\n",
        "    aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "            'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "            'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "            ]\n",
        "    aa_tokenizer = {aa_list[k]:k+1 for k in range(len(aa_list))}\n",
        "    aa_tokenizer['*'] = 0\n",
        "    aa_tokenizer['X'] = 0\n",
        "    # optionally handle B, J, Z ambiguities\n",
        "    # Asx\tB\tAspartic acid or Asparagine (D or N)\n",
        "    # Glx\tZ\tGlutamic acid or Glutamine (E or Q)\n",
        "    # Xaa\tX\tAny amino acid\n",
        "    # Xle\tJ\tLeucine or Isoleucine (L or I)\n",
        "    aa_tokenizer['B'] = 0\n",
        "    aa_tokenizer['Z'] = 0\n",
        "    aa_tokenizer['J'] = 0\n",
        "\n",
        "    return np.vectorize(aa_tokenizer.get)(data)\n",
        "\n",
        "aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "        'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "        'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-G4mKW85oKz"
      },
      "source": [
        "#Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO2ZnY5VNmuC"
      },
      "source": [
        "##Corona (Multi-Genus) Sequence Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the csv file with coronavirus sequences (different species)"
      ],
      "metadata": {
        "id": "9KXFynDzUe_C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40aw2aurV8dQ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(FILELOC + \"coronavirus_spike/\" + \"coronataxonomy_dataset_notpreprocessed.csv\")\n",
        "print(len(data))\n",
        "\n",
        "# rename column with sequences to \"Spike\" for consistency with downstream operations\n",
        "data.rename(columns={'Seq':'Spike'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove short sequences or those whose species is not coronavirus"
      ],
      "metadata": {
        "id": "2_QGzf1AUlXi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVSUSZWSWVMA"
      },
      "outputs": [],
      "source": [
        "dataset = data[(data.Length>1000) & (data.Species.str.contains('coronavirus'))].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQZpHPFru6Pc"
      },
      "source": [
        "Create labels if classifying for host (human / non-human)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-iZCINgY6_J"
      },
      "outputs": [],
      "source": [
        "dataset['hostlabel'] = dataset.Host.apply(lambda x: 1 if x=='Homo sapiens' else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbuoG-x65weJ"
      },
      "source": [
        "Create labels if classifying by genus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgaptcB8Eqxa"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    if x=='Alphacoronavirus':\n",
        "        return 0\n",
        "    if x=='Betacoronavirus':\n",
        "        return 1\n",
        "    if x=='Gammacoronavirus':\n",
        "        return 2\n",
        "    if x=='Deltacoronavirus':\n",
        "        return 3\n",
        "\n",
        "dataset['genuslabel'] = dataset.Genus.apply(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display histogram of sequence lengths and set the maximum sequence length (SEQLEN)"
      ],
      "metadata": {
        "id": "awSlMynpVELJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5YYy5Cz5TTQ"
      },
      "outputs": [],
      "source": [
        "dataset.Spike.apply(len).plot.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set sequence length (pad shorter sequences / truncate longer sequences)"
      ],
      "metadata": {
        "id": "XyyRmI5MgH_R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVq7j7X8p_5L"
      },
      "outputs": [],
      "source": [
        "SEQLEN = 1500"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample dates may indicate that a sample was collected after SARS-CoV-2 is discovered. For some validation (i.e. determining whether SARS-CoV-2 is classified correctly as a Betacoronavirus) may want to remove post Jan 2020 samples. Additional code may be added below to remove those samples."
      ],
      "metadata": {
        "id": "utGMhbHPUx_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzYSbQ3Al9Bw"
      },
      "outputs": [],
      "source": [
        "dataset['date'] = dataset['Release_Date'].apply(lambda x:dateparse(x))\n",
        "dataset['date'] = dataset['date'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset[dataset.Collection_Date.isna()])) # some dates may be nandatadf['seqlen'] = datadf.Spike.apply(len)"
      ],
      "metadata": {
        "id": "aX4QVyIhh33u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sequences and create training and test data sets"
      ],
      "metadata": {
        "id": "LGggSkc-htFu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB5fHqICEAtX"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(dataset, 'Spike', SEQLEN)\n",
        "\n",
        "# the code below defines the labels as genus label\n",
        "# can be modified to define labels as host label, or as both\n",
        "y = dataset.genus.values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.8*len(seqtok)), replace=False)\n",
        "# testindex = np.array([k for k in range(len(seqtok)) if k not in trainindex])\n",
        "\n",
        "# save train index for future use:\n",
        "# np.savetxt(FILELOC + 'corona_trainindex.csv', trainindex, fmt='%i', delimiter=',')\n",
        "trainindex = np.loadtxt(FILELOC + 'corona_trainindex.csv', dtype=int, delimiter=',')\n",
        "\n",
        "xtrain = seqtok[trainindex]\n",
        "xtest = seqtok[testindex];\n",
        "NVARS = 0       # there are no additional variables besides the sequence\n",
        "\n",
        "ytrain = y[trainindex]\n",
        "ytest = y[testindex]"
      ],
      "metadata": {
        "id": "YryWUkYoiwCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSHFqfg0aV7"
      },
      "source": [
        "##SARS-CoV-2 Lineage Sequence Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Bncwe5TFG7"
      },
      "source": [
        "###Raw Sequences (random sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read a file with raw sequences (i.e. not aligned) used to demonstrate lineage prediction. These need to be generated by downloading sequences from GISAID because they cannot be separately distributed."
      ],
      "metadata": {
        "id": "TjOPkSKRVNzh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DMv1IQVS3Zd"
      },
      "outputs": [],
      "source": [
        "# Specific code used to generate a set of raw samples from a pre-existing\n",
        "# dataframe of sequences, sequence IDs, and date of first collection\n",
        "\n",
        "# with open(FILELOC + 'spike_reldate_0303.pkl', 'rb') as f:\n",
        "#     df = pickle.load(f)\n",
        "# df.reset_index(drop=False, inplace=True)\n",
        "# df = df[df.Lineage!=\"None\"].reset_index(drop=True)\n",
        "# df_sample = pd.concat([df[df.Count >= 100].sample(4000), df[df.Count.between(10,99)].sample(12000),\n",
        "#                        df[df.Count.between(2,3)].sample(3000), df[df.Count==1].sample(1000)], axis=0)\n",
        "# with open(f'{FILELOC}coronavirus_spike_sars2cov_rawsample.pkl', 'wb') as f:\n",
        "#     pickle.dump(df_sample, f)\n",
        "\n",
        "with open(f'{FILELOC}coronavirus_spike_sars2cov_rawsample.pkl', 'rb') as f:\n",
        "    datadf = pickle.load(f)\n",
        "datadf.reset_index(drop=False, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign lineages to labels (this can be readily modified)"
      ],
      "metadata": {
        "id": "G1Sqy702VWvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsgStjqS1kOR"
      },
      "outputs": [],
      "source": [
        "SEQLEN = 1500   # set sequence length (pad shorter sequences / truncate longer sequences)\n",
        "\n",
        "labelmap = {'AY.4':0, 'B.1.617.2':0,\n",
        "            'B.1':1, 'B.1.177':1, 'B.1.1':1, 'B.1.2':1,\n",
        "            'BA.1':2,\n",
        "            'BA.1.1':3,\n",
        "            'BA.2':4,\n",
        "            'P.1':5,\n",
        "            'B.1.351':6,\n",
        "            'B.1.427':7, 'B.1.429':7,\n",
        "            }\n",
        "\n",
        "datadf['Label'] = datadf['Lineage'].map(labelmap)\n",
        "datadf = datadf[datadf.Label.notna()].reset_index(drop=True)\n",
        "datadf['Label'] = datadf['Label'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sequences and create training and test data sets"
      ],
      "metadata": {
        "id": "ZPa-_pefi_gU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAncazbui_gV"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(dataset, 'Spike', SEQLEN)\n",
        "\n",
        "# the code below defines the labels as genus label\n",
        "# can be modified to define labels as host label, or as both\n",
        "y = dataset.genus.values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.8*len(seqtok)), replace=False)\n",
        "# testindex = np.array([k for k in range(len(seqtok)) if k not in trainindex])\n",
        "\n",
        "# save train index for future use:\n",
        "# np.savetxt(FILELOC + 'corona_trainindex.csv', trainindex, fmt='%i', delimiter=',')\n",
        "trainindex = np.loadtxt(FILELOC + 'corona_trainindex.csv', dtype=int, delimiter=',')\n",
        "\n",
        "xtrain = seqtok[trainindex]\n",
        "xtest = seqtok[testindex];\n",
        "NVARS = 0       # there are no additional variables besides the sequence\n",
        "\n",
        "ytrain = y[trainindex]\n",
        "ytest = y[testindex]"
      ],
      "metadata": {
        "id": "sbYDLH_wi_gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0h33Z0RTIJ3"
      },
      "source": [
        "###Aligned Sequences (covid-patient dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read a file with aligned SARS-CoV-2 sequences to predict lineage or date of first occurrence. In this case, the data were originally generated for clinical severity prediction, and are then processed to generate distinct sequences (i.e. remove repeated sequences from the database) and assign them a date of first occurence and lineage."
      ],
      "metadata": {
        "id": "s2XzzGhHbV7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHxABGCYTEOz"
      },
      "outputs": [],
      "source": [
        "# seqs = pd.read_csv(FILELOC + 'covid_patient_seqs_20220228/covid_patient_seqs_20220228.csv')\n",
        "# seqs.drop(columns=\"Spike\", inplace=True)\n",
        "\n",
        "# df = seqs.copy()\n",
        "# df['Country'] = df['Location'].apply(lambda x: x.split('/')[1].strip())\n",
        "# REFDATE = '2019-12-01'; refdt = dateparse(REFDATE)\n",
        "# def f(x):\n",
        "#     return (dateparse(x) - refdt).days\n",
        "# df['Date'] = df['Collection date'].apply(f)\n",
        "# df = df[['SequenceID', 'Date', 'Clade', 'Lineage', 'Country', 'MaskedSeq', 'Collection date', 'Location']]\n",
        "\n",
        "# df = df[df.Date > 0] # get rid of negative date values\n",
        "\n",
        "# df_reldate = df.groupby(\"MaskedSeq\")[\"Date\"].apply(list).to_frame()\n",
        "# df_reldate['First_Date'] = df_reldate.Date.apply(np.amin)\n",
        "# df_reldate['Last_Date'] = df_reldate.Date.apply(np.amax)\n",
        "# df_reldate['Peak_Date'] = df_reldate.Date.apply(lambda x: np.median(np.argwhere(np.bincount(x)==np.amax(np.bincount(x)))))\n",
        "# df_reldate['Count'] = df_reldate.Date.apply(len)\n",
        "# max_first_date = max(df_reldate.First_Date)\n",
        "# df_reldate['relfirstdate'] = df_reldate['First_Date'].apply(lambda x: x/max_first_date)\n",
        "# df_out = df_reldate.join(df.groupby(\"MaskedSeq\")[\"Location\"].apply(list).to_frame())\n",
        "# def f(x):\n",
        "#     try:\n",
        "#         return pd.Series.mode(x)[0]\n",
        "#     except:\n",
        "#         return np.nan\n",
        "\n",
        "# df_out = df_out.join(df.groupby(\"MaskedSeq\")[\"Clade\"].agg(f).to_frame())\n",
        "# df_out = df_out.join(df.groupby(\"MaskedSeq\")[\"Lineage\"].agg(f).to_frame())\n",
        "\n",
        "# with open(FILELOC + 'covid_patient_seqs_20220228/covid_patient_seqs_grouped_20220228.pkl', 'wb') as f:\n",
        "#     pickle.dump(df_out, f)\n",
        "\n",
        "with open(FILELOC + 'covid_patient_seqs_20220228/covid_patient_seqs_grouped_20220228.pkl', 'rb') as f:\n",
        "    datadf = pickle.load(f)\n",
        "datadf.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is used to assign labels based on lineage."
      ],
      "metadata": {
        "id": "1axurB8Ybo8f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CiP_6BUNw6q"
      },
      "outputs": [],
      "source": [
        "# rename column with sequences to \"Spike\" for consistency with downstream operations\n",
        "datadf.rename(columns={'MaskedSeq':'Spike'}, inplace=True)\n",
        "\n",
        "datadf = datadf[datadf.Count >= 2].reset_index(drop=True)\n",
        "\n",
        "labelmap = {'AY.4':0, 'B.1.617.2':0,\n",
        "            'B.1':1, 'B.1.177':1, 'B.1.1':1, 'B.1.2':1,\n",
        "            'BA.1':2,\n",
        "            'BA.1.1':3,\n",
        "            'BA.2':4,\n",
        "            'P.1':5,\n",
        "            'B.1.351':6,\n",
        "            'B.1.427':7, 'B.1.429':7,\n",
        "            }\n",
        "\n",
        "datadf['Label'] = datadf['Lineage'].map(labelmap)\n",
        "datadf = datadf[datadf.Label.notna()].reset_index(drop=True)\n",
        "datadf['Label'] = datadf['Label'].astype(int)\n",
        "\n",
        "# aligned sequence lengths are equally 1273\n",
        "SEQLEN = 1273"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sequences and create training and test data sets"
      ],
      "metadata": {
        "id": "D18tKZKOjAm2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH8dQErijAm3"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(dataset, 'Spike', SEQLEN)\n",
        "\n",
        "# the code below defines the labels as genus label\n",
        "# can be modified to define labels as host label, or as both\n",
        "y = dataset.genus.values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.8*len(seqtok)), replace=False)\n",
        "# testindex = np.array([k for k in range(len(seqtok)) if k not in trainindex])\n",
        "\n",
        "# save train index for future use:\n",
        "# np.savetxt(FILELOC + 'corona_trainindex.csv', trainindex, fmt='%i', delimiter=',')\n",
        "trainindex = np.loadtxt(FILELOC + 'corona_trainindex.csv', dtype=int, delimiter=',')\n",
        "\n",
        "xtrain = seqtok[trainindex]\n",
        "xtest = seqtok[testindex];\n",
        "NVARS = 0       # there are no additional variables besides the sequence\n",
        "\n",
        "ytrain = y[trainindex]\n",
        "ytest = y[testindex]"
      ],
      "metadata": {
        "id": "GcXTabMCjAm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JCe5_O_xIb4"
      },
      "source": [
        "###COVID-19 Disease Severity Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a file of sequences pre-processed as per other scripts"
      ],
      "metadata": {
        "id": "_XUK6KX8XuAE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkUMwl358gQw"
      },
      "outputs": [],
      "source": [
        "pdf = pd.read_csv(FILELOC + 'covid_patient_seqs_20220228/covid_patient_seqs_20220228.csv')\n",
        "\n",
        "print(len(pdf))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following code to generate/regenerate labels (commented out because the csv file loaded above already has labels)."
      ],
      "metadata": {
        "id": "a8s70ojaXz0c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQq0Y2mxBTVH"
      },
      "outputs": [],
      "source": [
        "# labelmap = {'alive' : -1,\n",
        "#             'asymptomatic' : 0,\n",
        "#             'dead' : 1,\n",
        "#             'hospitalized' : 1,\n",
        "#             'mild' : 0,\n",
        "#             'moderate' : 0,\n",
        "#             'released' : 1,\n",
        "#             'screening' : 0,\n",
        "#             'severe' : 1,\n",
        "#             'symptomatic' : -1,\n",
        "#             'unknown' : -1,\n",
        "#         }\n",
        "\n",
        "# pdf['Label'] = pdf['Category'].map(labelmap).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove samples with invalid labels and patient variables (which were set to -1 in preprocessing)."
      ],
      "metadata": {
        "id": "-WERzVmUX6K9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di5bnxOD_u3a"
      },
      "outputs": [],
      "source": [
        "pdf.drop(pdf[pdf.Label==-1].index, inplace=True)\n",
        "print(len(pdf))\n",
        "pdf.drop(pdf[pdf.Age==-1].index, inplace=True)\n",
        "print(len(pdf))\n",
        "pdf.drop(pdf[pdf.Gender==-1].index, inplace=True)\n",
        "print(len(pdf))\n",
        "pdf.reset_index(drop=True, inplace=True)\n",
        "\n",
        "pdf.drop(columns = 'Spike', inplace=True)\n",
        "\n",
        "# rename column with sequences to \"Spike\" for consistency with downstream operations\n",
        "pdf.rename(columns={'MaskedSeq':'Spike'}, inplace=True)\n",
        "# aligned sequence lengths are equally 1273\n",
        "SEQLEN = 1273"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define patient variables (age/gender/date of sample collection)"
      ],
      "metadata": {
        "id": "WhZ4XcLMgf3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTrHLhRV_u0b"
      },
      "outputs": [],
      "source": [
        "vars = pdf[['Age','Gender','reldate']].values\n",
        "# \"reldate\" is the sample collection date in days after December 1, 2019\n",
        "\n",
        "NVARS = 3\n",
        "NCLASSES = 2    # 2 outcomes (mild/severe)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sequence and create training and testing data"
      ],
      "metadata": {
        "id": "l4h6pru4gp13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seqtok = tokenize_sequences(pdf, 'Spike', SEQLEN)\n",
        "y = pdf.Label.values"
      ],
      "metadata": {
        "id": "oCJP2V-WXoB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.7*len(seqtok)), replace=False)\n",
        "# testindex = np.array([k for k in range(len(seqtok)) if k not in trainindex])\n",
        "\n",
        "# save train index for future use:\n",
        "# np.savetxt(FILELOC + 'covidpatient_trainindex.csv', trainindex, fmt='%i', delimiter=',')\n",
        "\n",
        "# load predefined set of training indices\n",
        "trainindex = np.loadtxt(FILELOC + 'covidpatient_trainindex.csv', dtype=int, delimiter=',')\n",
        "\n",
        "xtrain = [seqtok[trainindex], vars[trainindex]]\n",
        "xtest = [seqtok[testindex], vars[testindex]]\n",
        "ytrain = y[trainindex]\n",
        "ytest = y[testindex]"
      ],
      "metadata": {
        "id": "gg0mJ51IhFzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a preexisting file of patient data and sequences that has already been processed as above"
      ],
      "metadata": {
        "id": "FOz1RBBWXolv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBi3VwKWArwc"
      },
      "outputs": [],
      "source": [
        "# The file loaded below already has training and test sets defined\n",
        "\n",
        "with open (FILELOC + 'covid_patient_seqs_20220228/covid_patient_data_old.pkl', 'rb') as f:\n",
        "    traindf, testdf = pickle.load(f)\n",
        "print(len(traindf), len(testdf))\n",
        "\n",
        "# rename column with sequences to \"Spike\" for consistency\n",
        "traindf.rename(columns={'ISM':'Spike'}, inplace=True)\n",
        "testdf.rename(columns={'ISM':'Spike'}, inplace=True)\n",
        "\n",
        "NVARS = 3       # 3 patient variables (age/gender/date)\n",
        "NCLASSES = 2    # 2 outcomes (mild/severe)\n",
        "\n",
        "trainvars = traindf[['age', 'gender', 'date']].values\n",
        "testvars = testdf[['age', 'gender', 'date']].values\n",
        "\n",
        "SEQLEN = 1273"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize sequences and define \"xtrain/xtest/ytrain/ytest\" consistently with other methods for use in training and validation below."
      ],
      "metadata": {
        "id": "RgX-vGT2jDzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIskmxP2BpiD"
      },
      "outputs": [],
      "source": [
        "trainseq = tokenize_sequences(traindf, 'Spike', SEQLEN)\n",
        "testseq = tokenize_sequences(testdf, 'Spike', SEQLEN)\n",
        "\n",
        "xtrain = [trainseq, trainvars]\n",
        "xtest = [testseq, testvars]\n",
        "ytrain = traindf.Label.values\n",
        "ytest = testdf.Label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiRleV1h3QHw"
      },
      "source": [
        "#Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naBxTAsNFVx1"
      },
      "source": [
        "##Class Balancing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2D1JqUZgsxf"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = list(class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                       classes=np.arange(NCLASSES), y=ytrain))\n",
        "\n",
        "# optionally, each sample may be weighted individually\n",
        "sample_weights = np.array([class_weights[int(yi)] for yi in ytrain])\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhOtMFUdCMMN"
      },
      "source": [
        "##Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This training routine operates in one shot with a preset number of epochs. Early stopping is optional."
      ],
      "metadata": {
        "id": "F6uGRVLxQ8kw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qNvsD6U7Ej5"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 100\n",
        "# A larger batch size more optimally uses TPU resources\n",
        "BATCH_SIZE = 48*8 # 48*4\n",
        "# VAL_SPLIT = 0.2   # use if defining early stopping callbacks with validation data\n",
        "\n",
        "# See Tensorflow documentation for how to modify the early stopping callback\n",
        "\n",
        "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor = 'loss',\n",
        "#     verbose = 1,\n",
        "#     patience = 10, #5,\n",
        "#     mode = 'auto',\n",
        "#     min_delta = 0,\n",
        "#     restore_best_weights = True\n",
        "#     )\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "if tpu_env:\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model(regress=False, singleclass=True, multiclass=False,\n",
        "                            output_multiheadatt=False, use_att=True, nclasses=NCLASSES,\n",
        "                            output_two=False, numvars=3, mask=True)\n",
        "else:\n",
        "    model = reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                        output_multiheadatt=False, use_att=True, nclasses=NCLASSES,\n",
        "                        numvars=NVARS, mask=False)\n",
        "    \n",
        "# Output table of deep model layers and parameters / connections\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(xtrain, ytrain,\n",
        "                    # sample_weight = sample_weights,\n",
        "                    # use the following instead of sample_weight if weighting samples individually\n",
        "                    class_weight = {c:class_weights[c] for c in range(NCLASSES)},\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS,\n",
        "                    verbose = 1,\n",
        "                    # validation_split = VAL_SPLIT,\n",
        "                    # callbacks = [early_stopping],\n",
        "                    )\n",
        "\n",
        "model.save_weights(f\"{FILELOC}weights_sarscov2_20220408.h5\", save_format='h5', overwrite=True)\n",
        "\n",
        "# to save weights of the model\n",
        "# model.save_weights(f\"{FILELOC}taxonomy_weights.h5\", save_format='h5', overwrite=True)\n",
        "# np.savetxt(FILELOC + 'corona_trainindex_sars2cov_20220331.csv', trainindex, fmt='%i', delimiter=',')\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2_20220331.h5\", save_format='h5', overwrite=True)\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2_20220331_nodense.h5\", save_format='h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output confusion matrix and sklearn classification report (precision, recall, f1-score)"
      ],
      "metadata": {
        "id": "Y04OICR2RDa_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LWbMxJyxTg_"
      },
      "outputs": [],
      "source": [
        "print(\"Results for Testing Data:\")\n",
        "test_predict = model.predict(xtest)\n",
        "test_predict_bool = np.round(test_predict)\n",
        "\n",
        "# Use for single-class\n",
        "ClassRep = classification_report(ytest, test_predict_bool)\n",
        "ConfMatrix = confusion_matrix(ytest, test_predict_bool)\n",
        "\n",
        "# Use for multiclass\n",
        "# ClassRep = classification_report(ytest, test_predict_bool.argmax(axis=1))\n",
        "# ConfMatrix = confusion_matrix(ytest, test_predict_bool.argmax(axis=1))\n",
        "\n",
        "# Use for multilabel\n",
        "# ConfMatrix = multilabel_confusion_matrix(ytest, test_predict_bool)\n",
        "\n",
        "print(ClassRep)\n",
        "print(ConfMatrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAerrFVqkKwc"
      },
      "source": [
        "##Load Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The arguments for reset_model() should be the same as used for model training."
      ],
      "metadata": {
        "id": "wU06p34bkKwd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx1n-pOskKwd"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "if tpu_env:\n",
        "    with tpu_strategy.scope():\n",
        "        model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                        output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "        model.load_weights(f\"{FILELOC}taxonomy_weights_20220112_seq1000_{run}.h5\")\n",
        "        model.compile()\n",
        "else:\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    model.load_weights(f\"{FILELOC}taxonomy_weights_20220112_seq1000_{run}.h5\")\n",
        "    model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load evaluation data and tokenize sequences\n",
        "\n",
        "# Use Dataset Proeprocessing section of notebook to define an appropriate\n",
        "# set of evaluation data, i.e., a dataframe with the name \"tdf\"\n",
        "\n",
        "tok = tokenize_sequences(tdf, 'Spike', SEQLEN)"
      ],
      "metadata": {
        "id": "Pu6155u7kdY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sample code below generates embedding data, attention data, and predictions and loads them in a dataframe with evaluation data"
      ],
      "metadata": {
        "id": "w4BkLahOk7ka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok = tokenize_sequences(tdf,'MaskedSeq',1500)\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()\n",
        "    pred = model.predict(tok)\n",
        "    emb = get_embedding_model.predict(tok)\n",
        "    att = get_attention_model.predict(tok)\n",
        "    tdf['pred'] = [p for p in pred]\n",
        "    tdf['emb'] = [e for e in emb]\n",
        "    tdf['att'] = [a for a in att]"
      ],
      "metadata": {
        "id": "RfruWW71kcTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5iQD5bVSZ8e"
      },
      "source": [
        "###Plot Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample code to plot attention"
      ],
      "metadata": {
        "id": "8soAXtYOlM2g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL7DSx-zYssA"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(16,6)\n",
        "\n",
        "# att_plot = np.median(att,axis=0)\n",
        "# ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, color='black')\n",
        "\n",
        "att_plot = att[565,:]/np.median(att[565,:])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='attention')\n",
        "att_plot = mha[0]/np.median(mha[0])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-0')\n",
        "att_plot = mha[4]/np.median(mha[7])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-4')\n",
        "att_plot = mha[7]/np.median(mha[7])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-8')\n",
        "\n",
        "plt.ylim(0,10)\n",
        "\n",
        "# for k in [0,1000,11000]:\n",
        "#     ax.plot(range(1,SEQLEN+1), att[k], linewidth=1, label=k)\n",
        "\n",
        "ax.set_xlabel('Sequence Position', fontsize=18, fontweight='bold')\n",
        "ax.set_ylabel('Attention', fontsize=18, fontweight='bold')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHgSiUFo-3R7"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(16,6)\n",
        "\n",
        "# att_plot = np.median(att,axis=0)\n",
        "# ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, color='black')\n",
        "\n",
        "att_plot = att[565,:]/np.median(att[565,:])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='attention')\n",
        "att_plot = mha[0]/np.median(mha[0])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-0')\n",
        "att_plot = mha[4]/np.median(mha[7])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-4')\n",
        "att_plot = mha[7]/np.median(mha[7])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-8')\n",
        "\n",
        "plt.ylim(0,10)\n",
        "\n",
        "# for k in [0,1000,11000]:\n",
        "#     ax.plot(range(1,SEQLEN+1), att[k], linewidth=1, label=k)\n",
        "\n",
        "ax.set_xlabel('Sequence Position', fontsize=18, fontweight='bold')\n",
        "ax.set_ylabel('Attention', fontsize=18, fontweight='bold')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tZmeFJGYv0Y"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(16,6)\n",
        "\n",
        "att_plot = np.median(att,axis=0)\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, color='black')\n",
        "\n",
        "ax.set_xlabel('Sequence Position', fontsize=18, fontweight='bold')\n",
        "ax.set_ylabel('Attention', fontsize=18, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnnU8rd2_UGl"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(16,6)\n",
        "\n",
        "att_plot = np.median(att,axis=0)\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, color='black')\n",
        "\n",
        "ax.set_xlabel('Sequence Position', fontsize=18, fontweight='bold')\n",
        "ax.set_ylabel('Attention', fontsize=18, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLaLcXnTZ60T"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(16,6)\n",
        "\n",
        "# att_plot = np.median(att,axis=0)\n",
        "# ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, color='black')\n",
        "\n",
        "att_plot = att[565,:]/np.median(att[565,:])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='attention')\n",
        "att_plot = mha[0]/np.median(mha[0])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-0')\n",
        "att_plot = mha[4]/np.median(mha[7])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-4')\n",
        "att_plot = mha[7]/np.median(mha[7])\n",
        "ax.plot(range(1,SEQLEN+1), att_plot, linewidth=1, label='MH-8')\n",
        "\n",
        "# for k in [0,1000,11000]:\n",
        "#     ax.plot(range(1,SEQLEN+1), att[k], linewidth=1, label=k)\n",
        "\n",
        "ax.set_xlabel('Sequence Position', fontsize=18, fontweight='bold')\n",
        "ax.set_ylabel('Attention', fontsize=18, fontweight='bold')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgRUWNPjaCJ_"
      },
      "outputs": [],
      "source": [
        "b\n",
        "np.where(att_plot > 0.0009)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7GNxtZzaCHB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kARw7DUzNfWE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXjnNBtE8f8S"
      },
      "outputs": [],
      "source": [
        "sdf = pd.read_csv(FILELOC + \"species_dataset_20211127.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0eVRRo8hKb"
      },
      "outputs": [],
      "source": [
        "run = 1\n",
        "tf.keras.backend.clear_session()\n",
        "with tpu_strategy.scope():\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    model.load_weights(f\"{FILELOC}taxonomy_weights_20220112_seq1000_{run}.h5\")\n",
        "    model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRJWe8_a8jBe"
      },
      "outputs": [],
      "source": [
        "tok = tokenize_sequences(sdf,'Seq',1500)\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()\n",
        "    pred = model.predict(tok)\n",
        "    emb = get_embedding_model.predict(tok)\n",
        "    att = get_attention_model.predict(tok)\n",
        "    sdf['pred'] = [p for p in pred]\n",
        "    sdf['emb'] = [e for e in emb]\n",
        "    sdf['att'] = [a for a in att]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5Qt35ibnYfF"
      },
      "outputs": [],
      "source": [
        "# with open(FILELOC + 'topsequences_taxonomy_20220112.pkl', 'wb') as f:\n",
        "#     pickle.dump(tdf, f)\n",
        "# with open(FILELOC + 'topsequences_taxonomy_20220112.pkl', 'rb') as f:\n",
        "#     tdf = pickle.load(f)\n",
        "\n",
        "# with open(FILELOC + 'topsequences_taxonomy_20220112_seq1000.pkl', 'wb') as f:\n",
        "#     pickle.dump(tdf, f)\n",
        "with open(FILELOC + 'topsequences_taxonomy_20220112_seq1000.pkl', 'rb') as f:\n",
        "    tdf = pickle.load(f)\n",
        "\n",
        "# with open(FILELOC + 'allsequences_taxonomy_20220112_seq1000.pkl', 'wb') as f:\n",
        "    # pickle.dump(sdf, f)\n",
        "with open(FILELOC + 'allsequences_taxonomy_20220112_seq1000.pkl', 'rb') as f:\n",
        "    sdf = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hz2ZWeh8Z7k"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwYOzHRK75wv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g169PWV75tH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs-KcpqA75pu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlMkXKhy75mb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im-VigyF75jv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SAu3OhhyC87"
      },
      "outputs": [],
      "source": [
        " from collections import Counter\n",
        "\n",
        "# error = {}\n",
        "# for run in range(1,11):\n",
        "for run in [9]:\n",
        "    error[run] = {}\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    with tpu_strategy.scope():\n",
        "        model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                        output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "        # model.load_weights(f\"{FILELOC}taxonomy_weights_20211213_{run}.h5\")\n",
        "        model.load_weights(f\"{FILELOC}taxonomy_weights_20211221_{run}.h5\")        \n",
        "        model.compile()\n",
        "\n",
        "        vdf = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_valid1001_raw_sequences.csv')\n",
        "        vdf.drop(columns='sample_weight',inplace=True)\n",
        "        valtok = tokenize_sequences(vdf, SeqCol='ISM', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Patient Validation Data\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)]\n",
        "\n",
        "        vdf = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_raw_sequences.csv')\n",
        "        vdf.drop(columns='sample_weight',inplace=True)\n",
        "        valtok = tokenize_sequences(vdf, SeqCol='ISM', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Patient Training Data\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)]\n",
        "\n",
        "        vdf = rawsample\n",
        "        valtok = tokenize_sequences(vdf, 'Spike', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Raw Sample\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)]\n",
        "\n",
        "        vdf = rawshort\n",
        "        valtok = tokenize_sequences(vdf, 'Spike', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Raw Short Sequences\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)]\n",
        "\n",
        "        vdf = rawnoise\n",
        "        valtok = tokenize_sequences(vdf, 'Spike', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Raw Noisy Sequences\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)] \n",
        "\n",
        "datatypes = list(error[1].keys())\n",
        "mean_error = {d:np.mean([error[k][d][0] for k in range(1,11)]) for d in datatypes}\n",
        "std_error = {d:np.std([error[k][d][0] for k in range(1,11)]) for d in datatypes}\n",
        "\n",
        "print(\"Dataset   |    Error    |    Dataset size\")\n",
        "for run in range(1,11):\n",
        "    print(f\"Run {run}\")\n",
        "    for e,v in error[run].items():\n",
        "        print(f\"{e}  |  {v[0]}  |  {v[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v182xLislnnH"
      },
      "outputs": [],
      "source": [
        "with open(FILELOC + \"covid_taxonomy_error_trans_20211221.pkl\", \"wb\") as f:\n",
        "    pickle.dump([error, datatypes, mean_error, std_error], f)\n",
        "# with open(FILELOC + \"covid_taxonomy_error_trans.pkl\", \"wb\") as f:\n",
        "#     pickle.dump([error, datatypes, mean_error, std_error], f)\n",
        "# with open(FILELOC + \"covid_taxonomy_error_trans.pkl\", \"rb\") as f:\n",
        "#     error, datatypes, mean_error, std_error = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySSPcwCxQWZ0"
      },
      "outputs": [],
      "source": [
        "for r in error:\n",
        "    print(r, error[r]['Raw Short Sequences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGWeui39QWWn"
      },
      "outputs": [],
      "source": [
        "std_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnQuSLzSJ9NK"
      },
      "outputs": [],
      "source": [
        "with open(f'{FILELOC}coronovirus_spike_taxonomy_sequence_samples.pkl', 'rb') as f:\n",
        "    rawsample, rawshort, rawnoise = pickle.load(f)\n",
        "pat_val = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_valid1001_raw_sequences.csv')\n",
        "pat_train = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_raw_sequences.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRPQow3NOoS4"
      },
      "outputs": [],
      "source": [
        "run = 8\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "with tpu_strategy.scope():\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    # model.load_weights(f\"{FILELOC}taxonomy_weights_20211213_{run}.h5\")\n",
        "    model.load_weights(f\"{FILELOC}taxonomy_weights_20211221_{run}.h5\")\n",
        "    model.compile()\n",
        "\n",
        "datatype = ['Patient Training Data', 'Patient Validation Data',\n",
        "            'Raw Noisy Sequences', 'Raw Sample', 'Raw Short Sequences']\n",
        "datasrc = [pat_train, pat_val, rawnoise, rawsample, rawshort]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76XNvCDXJyZJ"
      },
      "outputs": [],
      "source": [
        "pred = {}; emb = {}; att = {}\n",
        "with tpu_strategy.scope():\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()\n",
        "    for d in range(len(datatype)):\n",
        "        if 'Patient' in datatype[d]:\n",
        "            tok = tokenize_sequences(datasrc[d], 'ISM', seqlen=1500)\n",
        "        else:\n",
        "            tok = tokenize_sequences(datasrc[d], 'Spike', seqlen=1500)\n",
        "        pred[datatype[d]] = model.predict(tok, verbose=True, batch_size=32*8)\n",
        "        emb[datatype[d]] = get_embedding_model.predict(tok, verbose=True, batch_size=32*8)\n",
        "        att[datatype[d]] = get_attention_model.predict(tok, verbose=True, batch_size=32*8)\n",
        "\n",
        "with open(FILELOC + 'covid_taxonomy_model_results_20211221.pkl', 'wb') as f:\n",
        "    pickle.dump([datatype, pred, emb, att], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSDcah6HSbgk"
      },
      "outputs": [],
      "source": [
        "# mha = {}\n",
        "# for h in range(8):\n",
        "#     mha[h] = np.sum(wts[0,h], axis=1)\n",
        "# mhadf = pd.DataFrame.from_dict({f'Head {h}':mha[h] for h in range(8)})\n",
        "# topdf = pd.DataFrame.from_dict({f'Head {h}':mhadf.sort_values(by=f'Head {h}', ascending=False).head(20).index for h in range(8)})\n",
        "# display(topdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKDzqgqSSiD6"
      },
      "outputs": [],
      "source": [
        "mha = {}\n",
        "for h in range(8):\n",
        "    mha[h] = np.sum(wts[0,h], axis=0)\n",
        "mhadf = pd.DataFrame.from_dict({f'Head {h}':mha[h] for h in range(8)})\n",
        "topdf = pd.DataFrame.from_dict({f'Head {h}':mhadf.sort_values(by=f'Head {h}', ascending=False).head(20).index for h in range(8)})\n",
        "display(topdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSkkSpKjSb-u"
      },
      "source": [
        "###Plot Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample code to plot and extract attention from transformer heads"
      ],
      "metadata": {
        "id": "LdBfpPd7lH0A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ma4aKsk5Sbrf"
      },
      "outputs": [],
      "source": [
        "get_mha_model = keras.Model(inputs=model.input,outputs=model.get_layer('multi_head_attention').output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M0SfJCqunpw"
      },
      "outputs": [],
      "source": [
        "labelmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjxNckjphOeB"
      },
      "outputs": [],
      "source": [
        "np.where(ytest==0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcf1reP8hQLt"
      },
      "outputs": [],
      "source": [
        "model.predict(xtest[197,:].reshape(1,-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-8TtLLIhJr4"
      },
      "outputs": [],
      "source": [
        "output,wts = get_mha_model.predict(xtest[565,:].reshape(1,-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NZHQPCSh_ft"
      },
      "outputs": [],
      "source": [
        "np.shape(wts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5K0fqmDwfUg"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import LogNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yjc9D3zSxqI4"
      },
      "outputs": [],
      "source": [
        "k = 0\n",
        "\n",
        "norm = {}\n",
        "for m in range(8):\n",
        "    norm[m] = LogNorm(vmin=np.min(np.min(wts[k,m])), vmax=np.max(np.max(wts[k,m])))\n",
        "\n",
        "fig,ax = plt.subplots(4,2)\n",
        "fig = fig.set_size_inches(18,18)\n",
        "a = [ax[0,0], ax[1,0], ax[2,0], ax[3,0],\n",
        "     ax[0,1], ax[1,1], ax[2,1], ax[3,1]]\n",
        "for m in range(8):\n",
        "    try:\n",
        "        sns.heatmap(wts[k,m], ax = a[m], norm = norm[m])\n",
        "    except:\n",
        "        pass\n",
        "# sns.heatmap(wts[k,0]/np.median(np.median(wts[k,0])), ax=ax[0,0])\n",
        "# sns.heatmap(wts[k,1]/np.median(np.median(wts[k,1])), ax=ax[1,0])\n",
        "# sns.heatmap(wts[k,2]/np.median(np.median(wts[k,2])), ax=ax[2,0])\n",
        "# sns.heatmap(wts[k,3]/np.median(np.median(wts[k,3])), ax=ax[3,0])\n",
        "# sns.heatmap(wts[k,4]/np.median(np.median(wts[k,4])), ax=ax[0,1])\n",
        "# sns.heatmap(wts[k,5]/np.median(np.median(wts[k,5])), ax=ax[1,1])\n",
        "# sns.heatmap(wts[k,6]/np.median(np.median(wts[k,6])), ax=ax[2,1])\n",
        "# sns.heatmap(wts[k,7]/np.median(np.median(wts[k,7])), ax=ax[3,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAGm4qgGWZrM"
      },
      "outputs": [],
      "source": [
        "k = 0\n",
        "\n",
        "norm = {}\n",
        "for m in range(8):\n",
        "    norm[m] = LogNorm(vmin=np.min(np.min(wts[k,m])), vmax=np.max(np.max(wts[k,m])))\n",
        "\n",
        "fig,ax = plt.subplots(4,2)\n",
        "fig = fig.set_size_inches(18,18)\n",
        "a = [ax[0,0], ax[1,0], ax[2,0], ax[3,0],\n",
        "     ax[0,1], ax[1,1], ax[2,1], ax[3,1]]\n",
        "for m in range(8):\n",
        "    sns.heatmap(wts[k,m], ax = a[m], norm = norm[m])\n",
        "\n",
        "# sns.heatmap(wts[k,0]/np.median(np.median(wts[k,0])), ax=ax[0,0])\n",
        "# sns.heatmap(wts[k,1]/np.median(np.median(wts[k,1])), ax=ax[1,0])\n",
        "# sns.heatmap(wts[k,2]/np.median(np.median(wts[k,2])), ax=ax[2,0])\n",
        "# sns.heatmap(wts[k,3]/np.median(np.median(wts[k,3])), ax=ax[3,0])\n",
        "# sns.heatmap(wts[k,4]/np.median(np.median(wts[k,4])), ax=ax[0,1])\n",
        "# sns.heatmap(wts[k,5]/np.median(np.median(wts[k,5])), ax=ax[1,1])\n",
        "# sns.heatmap(wts[k,6]/np.median(np.median(wts[k,6])), ax=ax[2,1])\n",
        "# sns.heatmap(wts[k,7]/np.median(np.median(wts[k,7])), ax=ax[3,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2uY_4PxSiGh"
      },
      "outputs": [],
      "source": [
        "k = 0\n",
        "\n",
        "norm = {}\n",
        "for m in range(8):\n",
        "    norm[m] = LogNorm(vmin=np.min(np.min(wts[k,m])), vmax=np.max(np.max(wts[k,m])))\n",
        "\n",
        "fig,ax = plt.subplots(4,2)\n",
        "fig = fig.set_size_inches(18,18)\n",
        "a = [ax[0,0], ax[1,0], ax[2,0], ax[3,0],\n",
        "     ax[0,1], ax[1,1], ax[2,1], ax[3,1]]\n",
        "for m in range(8):\n",
        "    sns.heatmap(wts[k,m], ax = a[m], norm = norm[m])\n",
        "\n",
        "# sns.heatmap(wts[k,0]/np.median(np.median(wts[k,0])), ax=ax[0,0])\n",
        "# sns.heatmap(wts[k,1]/np.median(np.median(wts[k,1])), ax=ax[1,0])\n",
        "# sns.heatmap(wts[k,2]/np.median(np.median(wts[k,2])), ax=ax[2,0])\n",
        "# sns.heatmap(wts[k,3]/np.median(np.median(wts[k,3])), ax=ax[3,0])\n",
        "# sns.heatmap(wts[k,4]/np.median(np.median(wts[k,4])), ax=ax[0,1])\n",
        "# sns.heatmap(wts[k,5]/np.median(np.median(wts[k,5])), ax=ax[1,1])\n",
        "# sns.heatmap(wts[k,6]/np.median(np.median(wts[k,6])), ax=ax[2,1])\n",
        "# sns.heatmap(wts[k,7]/np.median(np.median(wts[k,7])), ax=ax[3,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7_OLGmMA9Mq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9IE3wsxAyZJ"
      },
      "outputs": [],
      "source": [
        "mha = {}\n",
        "for h in range(8):\n",
        "    mha[h] = np.sum(wts[0,h], axis=0)\n",
        "mhadf = pd.DataFrame.from_dict({f'Head {h}':mha[h] for h in range(8)})\n",
        "topdf = pd.DataFrame.from_dict({f'Head {h}':mhadf.sort_values(by=f'Head {h}', ascending=False).head(30).index for h in range(8)})\n",
        "display(topdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3R5TOtQzPr_"
      },
      "outputs": [],
      "source": [
        "mha = {}\n",
        "for h in range(8):\n",
        "    mha[h] = np.sum(wts[0,h], axis=0)\n",
        "mhadf = pd.DataFrame.from_dict({f'Head {h}':mha[h] for h in range(8)})\n",
        "topdf = pd.DataFrame.from_dict({f'Head {h}':mhadf.sort_values(by=f'Head {h}', ascending=False).head(20).index for h in range(8)})\n",
        "display(topdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plot Embeddings"
      ],
      "metadata": {
        "id": "qstqdVorlhgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample code to plot TSNE of embeddings"
      ],
      "metadata": {
        "id": "o67yVGwvlfYp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR6Anne1en5K"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "t = TSNE(n_components=2).fit_transform(emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voC2dMwOe5e5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkT50gYsen1U"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(12,8)\n",
        "\n",
        "genus = ['Alphacoronavirus', 'Betacoronavirus', 'Gammacoronavirus', 'Deltacoronavirus']\n",
        "for genus_ind in np.unique(ytest):\n",
        "    selind = np.where(ytest==genus_ind)[0]\n",
        "    ax.scatter(t[selind,0], t[selind,1], marker='x', label=genus[genus_ind])\n",
        "ax.scatter(t[len(ytest):,0], t[len(ytest):,1], marker='x', label='Omicron')\n",
        "\n",
        "ax.legend(bbox_to_anchor=(1.0, 0.9),  framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2j1ChDpllmKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "24olAKH6lmFn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "x5iQD5bVSZ8e"
      ],
      "machine_shape": "hm",
      "name": "Corona_Transformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1hoyviiHrBiBrzq34P4ytxC9OEf0ao9Dw",
      "authorship_tag": "ABX9TyPPzqMPqjdzDoNpDhWWkKZz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}