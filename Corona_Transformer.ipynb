{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahrad/Covid/blob/main/Corona_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzkSry5x4VaL"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfXj-B_9RmZv"
      },
      "source": [
        "Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zLa737hB7e4_"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.parser import parse as dateparse\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, QuantileTransformer, OneHotEncoder, LabelBinarizer, OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RandomizedSearchCV, StratifiedShuffleSplit\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score,classification_report, make_scorer, balanced_accuracy_score, coverage_error, roc_auc_score, confusion_matrix, plot_confusion_matrix, multilabel_confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.utils import class_weight\n",
        "import sklearn as sk\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from imblearn.pipeline import make_pipeline,Pipeline\n",
        "\n",
        "# !python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8AvDkm_RpF2"
      },
      "source": [
        "If using Google colab, mount drive and set location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IiNZfPTu0_em"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "FILELOC = \"/content/drive/My Drive/COVID_Python/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOZ1irNHRrCv"
      },
      "source": [
        "Activate TPU resources if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "INmsA86dCdMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fdec78-7eab-4c8e-a7df-26860b3ea132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a TPU runtime.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    tpu_env=True\n",
        "except ValueError:\n",
        "    print('Not connected to a TPU runtime.')\n",
        "    tpu_env=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPgZv7LUzHDV"
      },
      "source": [
        "##Initialize Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MURAx4QiRtF2"
      },
      "source": [
        "Function to define the model.\n",
        "\n",
        "* \"regress\" sets the model to output a continuous value from 0 to 1 for regression.\n",
        "* \"singleclass\" is for a 0/1 binary output.\n",
        "* \"multiclass\" outputs a softmax for a number of classes. The code may be readily modified for multilabel classification as well.\n",
        "* \"output_multiheadatt\" generates the attention values for the transformer model, which may be extracted later.\n",
        "* \"use_att\" adds a flat sequence-wide attention layer.\n",
        "* \"output_two\" allows for two outputs, which maybe jointly used to compute the loss and optimize the model.\n",
        "* \"nclasses\" is the number of classes (ignored for regress or singleclass modes\n",
        "* \"mask\" allows for masking zeros in the sequence. \"numvars\" \n",
        "* \"numvars\" if set to a non-zero value adds additional variables as input (such as age/date/gender for clinical severity calculation) which are concatenated after the transformer and/or sequence-wide flat attention layer (if \"use_att\" is set to True) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3EfE13Hp8AJ9"
      },
      "outputs": [],
      "source": [
        "def reset_model(regress, singleclass, multiclass, output_multiheadatt, use_att, nclasses=4,\n",
        "                output_two=False, mask=True, numvars = 0):\n",
        "\n",
        "    if output_multiheadatt:\n",
        "        model_fn = AttMod_2\n",
        "    elif output_two:\n",
        "        model_fn = AttMod_3\n",
        "    else:\n",
        "        model_fn = AttModel\n",
        "\n",
        "    model = model_fn(L=SEQLEN,\n",
        "                     vocab_size=len(aa_list)+1,\n",
        "                     embdim = ENCDIM,\n",
        "                     numheads = NHEADS,\n",
        "                     ffdim = FFDIM,\n",
        "                     num_dense = NDENSE,\n",
        "                     mask_zero = mask,\n",
        "                     dropout_rate = DROPRATE,\n",
        "                     trans_drop = TRANSDROPRATE,\n",
        "                     Nt = NT,\n",
        "                     W = 1, Nc = NC, Nl = NL,\n",
        "                     regress=regress,\n",
        "                     singleclass=singleclass,\n",
        "                     multiclass=multiclass,\n",
        "                     use_att=use_att,\n",
        "                     nclasses=nclasses,\n",
        "                     nvars=numvars\n",
        "                     )\n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARN_RATE)\n",
        "    if regress:\n",
        "        loss = keras.losses.MeanSquaredError()\n",
        "        metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "            keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "            keras.losses.MeanAbsoluteError(name='mae')\n",
        "            ]\n",
        "    if singleclass:\n",
        "        loss = keras.losses.BinaryCrossentropy()\n",
        "        metrics = [keras.metrics.BinaryAccuracy(name='acc'),\n",
        "                   keras.metrics.AUC(name='auc')]\n",
        "    if multiclass:    \n",
        "        loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "        metrics = [keras.metrics.SparseCategoricalAccuracy(name='acc')]\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics,)\n",
        "                #   steps_per_execution = STEPS_PER_EXECUTION,)\n",
        "\n",
        "    if output_two:\n",
        "        losses = {'outfirst':'mean_squared_error',\n",
        "                  'outpeak':'mean_squared_error'}\n",
        "        lossweights = {'outfirst':1.0, 'outpeak':1.0}\n",
        "        metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "                   keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "                   keras.losses.MeanAbsoluteError(name='mae')]\n",
        "        model.compile(loss=losses, loss_weights=lossweights, optimizer=optimizer,metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGGOMX8bTDWt"
      },
      "source": [
        "##Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uzpJhreYnR70"
      },
      "outputs": [],
      "source": [
        "# These parameters are currently hard-coded\n",
        "ENCDIM = 1500\n",
        "NC = 300\n",
        "NL = 1                  # set to 0 to remove CNN pre-filtering\n",
        "NT = 1\n",
        "NHEADS = 4  # 8\n",
        "FFDIM = 64\n",
        "NDENSE = 64             # set to 0 to deactivate embedding layer\n",
        "TRANSDROPRATE = 0.4\n",
        "DROPRATE = 0.0\n",
        "\n",
        "LEARN_RATE = 0.0001\n",
        "\n",
        "BATCH_SIZE = 48\n",
        "\n",
        "STEPS_PER_EXECUTION = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOV554GHzJOd"
      },
      "source": [
        "##Model Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7aN_3QhTFek"
      },
      "source": [
        "Transformer and Token & Position Embedding definitions. Adapted from https://keras.io/examples/nlp/text_classification_with_transformer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dZrR1_I3KahE"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training, mask=None):\n",
        "        # attention masking currently not implemented\n",
        "        attn_output = self.att(inputs, inputs, attention_mask=None)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, mask_zero=False):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                output_dim=embed_dim,\n",
        "                                                mask_zero=mask_zero)\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,\n",
        "                                              mask_zero=mask_zero)\n",
        "        self.mask_zero = mask_zero\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "\n",
        "        if self.mask_zero:\n",
        "            mask = x._keras_mask\n",
        "            return x + positions, mask\n",
        "        else:\n",
        "            return x + positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0U-6K6EzO71"
      },
      "source": [
        "###Default model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KK05TjTo79ql"
      },
      "outputs": [],
      "source": [
        "def AttModel(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4, nvars=0):\n",
        "\n",
        "    inpSeq = keras.Input(shape=(L,))\n",
        "    if nvars > 0:\n",
        "        # additional variables besides sequence\n",
        "        inpVars = keras.Input(shape=(nvars,))\n",
        "    x = inpSeq\n",
        "    if nvars > 0:\n",
        "        v = inpVars\n",
        "\n",
        "    # if mask_zero:\n",
        "    #     x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    # inpSeq = keras.Input(shape=(L,))\n",
        "    # inpVars = keras.Input(shape=(3,))\n",
        "    # x = inpSeq\n",
        "    # v = inpVars\n",
        "\n",
        "    if mask_zero:\n",
        "        x, mask = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "    else:\n",
        "        x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x, mask_zero)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if nvars > 0:\n",
        "        # concatenate additional variables with the transformer output\n",
        "        h = keras.layers.concatenate([x, v])\n",
        "    else:\n",
        "        h = x\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(h)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    # model = keras.Model(inpTensor, finalOut)\n",
        "    if nvars > 0:\n",
        "        model = keras.Model([inpSeq,inpVars], finalOut)\n",
        "    else:\n",
        "        model = keras.Model(inpSeq, finalOut)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SirUALhzMiF"
      },
      "source": [
        "###Return multihead attention scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfWJYsvTTSir"
      },
      "source": [
        "if \"output_multiheadatt\" set to True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gPCCClpo7Rim"
      },
      "outputs": [],
      "source": [
        "def AttMod_2(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4, nvars=0):\n",
        "\n",
        "    inpSeq = keras.Input(shape=(L,))\n",
        "    if nvars > 0:\n",
        "        # additional variables besides sequence\n",
        "        inpVars = keras.Input(shape=(nvars,))\n",
        "    x = inpSeq\n",
        "    if nvars > 0:\n",
        "        v = inpVars\n",
        "\n",
        "    # if mask_zero:\n",
        "    #     x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    if mask_zero:\n",
        "        x, mask = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "    else:\n",
        "        x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # attention masking currently not implemented\n",
        "    y, attout = keras.layers.MultiHeadAttention(num_heads=numheads, key_dim=Nc,\n",
        "                                                )(x, x, return_attention_scores=True,\n",
        "                                                  attention_mask=None)\n",
        "    y = keras.layers.Dropout(trans_drop)(y)\n",
        "    z = keras.layers.LayerNormalization(epsilon=1e-6)(x + y)\n",
        "    z1 = keras.Sequential( [keras.layers.Dense(ffdim, activation=\"relu\"), keras.layers.Dense(embdim),])\n",
        "    z1 = keras.layers.Dropout(trans_drop)(z)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(z + z1)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if nvars > 0:\n",
        "        # concatenate additional variables with the transformer output\n",
        "        h = keras.layers.concatenate([x, v])\n",
        "    else:\n",
        "        h = x\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(h)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    # model = keras.Model(inpTensor, finalOut)\n",
        "    if nvars > 0:\n",
        "        model = keras.Model([inpSeq,inpVars], finalOut)\n",
        "    else:\n",
        "        model = keras.Model(inpSeq, finalOut)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P82fJycFzTnF"
      },
      "source": [
        "###Output two predictions for joint optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1YRmGhyTRR_"
      },
      "source": [
        "if \"output_two\" is set to True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hX-ogBerSt8P"
      },
      "outputs": [],
      "source": [
        "def AttMod_3(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4, nvars=0):\n",
        "\n",
        "    inpSeq = keras.Input(shape=(L,))\n",
        "    if nvars > 0:\n",
        "        # additional variables besides sequence\n",
        "        inpVars = keras.Input(shape=(nvars,))\n",
        "    x = inpSeq\n",
        "    if nvars > 0:\n",
        "        v = inpVars\n",
        "\n",
        "    # if mask_zero:\n",
        "    #     x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    if mask_zero:\n",
        "        x, mask = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "    else:\n",
        "        x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x, mask_zero)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if nvars > 0:\n",
        "        # concatenate additional variables with the transformer output\n",
        "        h = keras.layers.concatenate([x, v])\n",
        "    else:\n",
        "        h = x\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(h)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    out1 = keras.layers.Dense(1, activation='sigmoid', name='outfirst')(x)\n",
        "    out2 = keras.layers.Dense(1, activation='sigmoid', name='outpeak')(x)\n",
        "    # define the model's start and end points    \n",
        "    if nvars > 0:\n",
        "        model = keras.Model([inpSeq,inpVars], [out1,out2])\n",
        "    else:\n",
        "        model = keras.Model(inpSeq, [out1,out2])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcEFJHlTFMag"
      },
      "source": [
        "##Function to tokenize sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F-q4xrJCFMam"
      },
      "outputs": [],
      "source": [
        "def tokenize_sequences(data_dataframe, SeqCol='ISM', seqlen=1273):\n",
        "    def f(x):\n",
        "        if len(x) < seqlen:\n",
        "            return x + '*'*(seqlen-len(x))\n",
        "        elif len(x) > seqlen:\n",
        "            return x[:seqlen]\n",
        "        else:\n",
        "            return x\n",
        "    data = np.vstack(data_dataframe[SeqCol].apply(f).apply(lambda x: np.array(list(x))))\n",
        "    aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "            'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "            'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "            ]\n",
        "    aa_tokenizer = {aa_list[k]:k+1 for k in range(len(aa_list))}\n",
        "    aa_tokenizer['*'] = 0\n",
        "    aa_tokenizer['X'] = 0\n",
        "    # optionally handle B, J, Z ambiguities\n",
        "    # Asx\tB\tAspartic acid or Asparagine (D or N)\n",
        "    # Glx\tZ\tGlutamic acid or Glutamine (E or Q)\n",
        "    # Xaa\tX\tAny amino acid\n",
        "    # Xle\tJ\tLeucine or Isoleucine (L or I)\n",
        "    aa_tokenizer['B'] = 0\n",
        "    aa_tokenizer['Z'] = 0\n",
        "    aa_tokenizer['J'] = 0\n",
        "\n",
        "    return np.vectorize(aa_tokenizer.get)(data)\n",
        "\n",
        "aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "        'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "        'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-G4mKW85oKz"
      },
      "source": [
        "#Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsv49_c2_fSQ"
      },
      "source": [
        "Each of the modules (or submodules in the case of SARS-CoV-2 data) processes a different kind of data for different classification tasks. The modules all start with loading a datafile (which may have already been pre-processed) and end with the creation of numpy arrays for training, testing features and labels. Additional modules may be added for additional classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO2ZnY5VNmuC"
      },
      "source": [
        "##Corona (Multi-Genus) Sequence Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KXFynDzUe_C"
      },
      "source": [
        "Read the csv file with coronavirus sequences (different species)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40aw2aurV8dQ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(FILELOC + \"coronavirus_spike/\" + \"coronataxonomy_dataset_notpreprocessed.csv\")\n",
        "print(len(data))\n",
        "\n",
        "# rename column with sequences to \"Spike\" for consistency with downstream operations\n",
        "data.rename(columns={'Seq':'Spike'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_QGzf1AUlXi"
      },
      "source": [
        "Remove short sequences or those whose species is not coronavirus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVSUSZWSWVMA"
      },
      "outputs": [],
      "source": [
        "dataset = data[(data.Length>1000) & (data.Species.str.contains('coronavirus'))].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQZpHPFru6Pc"
      },
      "source": [
        "Create class labels (comment / comment-out code as needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-iZCINgY6_J"
      },
      "outputs": [],
      "source": [
        "# # Create labels for host: human/non-human classification task\n",
        "\n",
        "# NCLASSES = 2\n",
        "\n",
        "# dataset['hostlabel'] = dataset.Host.apply(lambda x: 1 if x=='Homo sapiens' else 0)\n",
        "\n",
        "# Create labels for genus classification task\n",
        "\n",
        "NCLASSES = 4\n",
        "\n",
        "def f(x):\n",
        "    if x=='Alphacoronavirus':\n",
        "        return 0\n",
        "    if x=='Betacoronavirus':\n",
        "        return 1\n",
        "    if x=='Gammacoronavirus':\n",
        "        return 2\n",
        "    if x=='Deltacoronavirus':\n",
        "        return 3\n",
        "\n",
        "dataset['genuslabel'] = dataset.Genus.apply(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awSlMynpVELJ"
      },
      "source": [
        "Display histogram of sequence lengths and set the maximum sequence length (SEQLEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5YYy5Cz5TTQ"
      },
      "outputs": [],
      "source": [
        "dataset.Spike.apply(len).plot.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyyRmI5MgH_R"
      },
      "source": [
        "Set sequence length (pad shorter sequences / truncate longer sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVq7j7X8p_5L"
      },
      "outputs": [],
      "source": [
        "SEQLEN = 1500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utGMhbHPUx_S"
      },
      "source": [
        "Sample dates may indicate that a sample was collected after SARS-CoV-2 is discovered. For some validation (i.e. determining whether SARS-CoV-2 is classified correctly as a Betacoronavirus) may want to remove post Jan 2020 samples. Additional code may be added below to remove those samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzYSbQ3Al9Bw"
      },
      "outputs": [],
      "source": [
        "dataset['date'] = dataset['Release_Date'].apply(lambda x:dateparse(x))\n",
        "dataset['date'] = dataset['date'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX4QVyIhh33u"
      },
      "outputs": [],
      "source": [
        "print(len(dataset[dataset.Collection_Date.isna()])) # some dates may be nandatadf['seqlen'] = datadf.Spike.apply(len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGggSkc-htFu"
      },
      "source": [
        "Tokenize sequences and create training and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB5fHqICEAtX"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(dataset, 'Spike', SEQLEN)\n",
        "\n",
        "# the code below defines the labels as genus label\n",
        "# can be modified to define labels as host label, or as both\n",
        "y = dataset.genus.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YryWUkYoiwCs"
      },
      "outputs": [],
      "source": [
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.8*len(seqtok)), replace=False)\n",
        "# testindex = np.array([k for k in range(len(seqtok)) if k not in trainindex])\n",
        "\n",
        "# save train index for future use:\n",
        "# np.savetxt(FILELOC + 'corona_trainindex.csv', trainindex, fmt='%i', delimiter=',')\n",
        "trainindex = np.loadtxt(FILELOC + 'corona_trainindex.csv', dtype=int, delimiter=',')\n",
        "\n",
        "xtrain = seqtok[trainindex]\n",
        "xtest = seqtok[testindex];\n",
        "NVARS = 0       # there are no additional variables besides the sequence\n",
        "\n",
        "ytrain = y[trainindex]\n",
        "ytest = y[testindex]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSHFqfg0aV7"
      },
      "source": [
        "##SARS-CoV-2 Lineage Sequence Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Bncwe5TFG7"
      },
      "source": [
        "###Raw Sequences (random sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjOPkSKRVNzh"
      },
      "source": [
        "Read a file with raw sequences (i.e. not aligned) used to demonstrate lineage prediction. These need to be generated by downloading sequences from GISAID because they cannot be separately distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DMv1IQVS3Zd"
      },
      "outputs": [],
      "source": [
        "# Specific code used to generate a set of raw samples from a pre-existing\n",
        "# dataframe of sequences, sequence IDs, and date of first collection\n",
        "\n",
        "# with open(FILELOC + 'spike_reldate_0303.pkl', 'rb') as f:\n",
        "#     df = pickle.load(f)\n",
        "# df.reset_index(drop=False, inplace=True)\n",
        "# df = df[df.Lineage!=\"None\"].reset_index(drop=True)\n",
        "# df_sample = pd.concat([df[df.Count >= 100].sample(4000), df[df.Count.between(10,99)].sample(12000),\n",
        "#                        df[df.Count.between(2,3)].sample(3000), df[df.Count==1].sample(1000)], axis=0)\n",
        "# with open(f'{FILELOC}coronavirus_spike_sars2cov_rawsample.pkl', 'wb') as f:\n",
        "#     pickle.dump(df_sample, f)\n",
        "\n",
        "with open(f'{FILELOC}coronavirus_spike_sars2cov_rawsample.pkl', 'rb') as f:\n",
        "    datadf = pickle.load(f)\n",
        "datadf.reset_index(drop=False, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1Sqy702VWvh"
      },
      "source": [
        "Assign lineages to labels (this can be readily modified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsgStjqS1kOR"
      },
      "outputs": [],
      "source": [
        "SEQLEN = 1500   # set sequence length (pad shorter sequences / truncate longer sequences)\n",
        "\n",
        "labelmap = {'AY.4':0, 'B.1.617.2':0,\n",
        "            'B.1':1, 'B.1.177':1, 'B.1.1':1, 'B.1.2':1,\n",
        "            'BA.1':2,\n",
        "            'BA.1.1':3,\n",
        "            'BA.2':4,\n",
        "            'P.1':5,\n",
        "            'B.1.351':6,\n",
        "            'B.1.427':7, 'B.1.429':7,\n",
        "            }\n",
        "\n",
        "datadf['Label'] = datadf['Lineage'].map(labelmap)\n",
        "datadf = datadf[datadf.Label.notna()].reset_index(drop=True)\n",
        "datadf['Label'] = datadf['Label'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPa-_pefi_gU"
      },
      "source": [
        "Tokenize sequences and create training and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAncazbui_gV"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(dataset, 'Spike', SEQLEN)\n",
        "\n",
        "# the code below defines the labels as genus label\n",
        "# can be modified to define labels as host label, or as both\n",
        "y = dataset.genus.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbYDLH_wi_gV"
      },
      "outputs": [],
      "source": [
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.8*len(seqtok)), replace=False)\n",
        "# testindex = np.array([k for k in range(len(seqtok)) if k not in trainindex])\n",
        "\n",
        "# save train index for future use:\n",
        "# np.savetxt(FILELOC + 'corona_trainindex.csv', trainindex, fmt='%i', delimiter=',')\n",
        "trainindex = np.loadtxt(FILELOC + 'corona_trainindex.csv', dtype=int, delimiter=',')\n",
        "\n",
        "xtrain = seqtok[trainindex]\n",
        "xtest = seqtok[testindex];\n",
        "NVARS = 0       # there are no additional variables besides the sequence\n",
        "\n",
        "ytrain = y[trainindex]\n",
        "ytest = y[testindex]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0h33Z0RTIJ3"
      },
      "source": [
        "###Aligned Sequences (covid-patient dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2XzzGhHbV7n"
      },
      "source": [
        "Read a file with aligned SARS-CoV-2 sequences to predict lineage or date of first occurrence. In this case, the data were originally generated for clinical severity prediction, and are then processed to generate distinct sequences (i.e. remove repeated sequences from the database) and assign them a date of first occurence and lineage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHxABGCYTEOz"
      },
      "outputs": [],
      "source": [
        "# seqs = pd.read_csv(FILELOC + 'covid_patient_seqs_20220228/covid_patient_seqs_20220228.csv')\n",
        "# seqs.drop(columns=\"Spike\", inplace=True)\n",
        "\n",
        "# df = seqs.copy()\n",
        "# df['Country'] = df['Location'].apply(lambda x: x.split('/')[1].strip())\n",
        "# REFDATE = '2019-12-01'; refdt = dateparse(REFDATE)\n",
        "# def f(x):\n",
        "#     return (dateparse(x) - refdt).days\n",
        "# df['Date'] = df['Collection date'].apply(f)\n",
        "# df = df[['SequenceID', 'Date', 'Clade', 'Lineage', 'Country', 'MaskedSeq', 'Collection date', 'Location']]\n",
        "\n",
        "# df = df[df.Date > 0] # get rid of negative date values\n",
        "\n",
        "# df_reldate = df.groupby(\"MaskedSeq\")[\"Date\"].apply(list).to_frame()\n",
        "# df_reldate['First_Date'] = df_reldate.Date.apply(np.amin)\n",
        "# df_reldate['Last_Date'] = df_reldate.Date.apply(np.amax)\n",
        "# df_reldate['Peak_Date'] = df_reldate.Date.apply(lambda x: np.median(np.argwhere(np.bincount(x)==np.amax(np.bincount(x)))))\n",
        "# df_reldate['Count'] = df_reldate.Date.apply(len)\n",
        "# max_first_date = max(df_reldate.First_Date)\n",
        "# df_reldate['relfirstdate'] = df_reldate['First_Date'].apply(lambda x: x/max_first_date)\n",
        "# df_out = df_reldate.join(df.groupby(\"MaskedSeq\")[\"Location\"].apply(list).to_frame())\n",
        "# def f(x):\n",
        "#     try:\n",
        "#         return pd.Series.mode(x)[0]\n",
        "#     except:\n",
        "#         return np.nan\n",
        "\n",
        "# df_out = df_out.join(df.groupby(\"MaskedSeq\")[\"Clade\"].agg(f).to_frame())\n",
        "# df_out = df_out.join(df.groupby(\"MaskedSeq\")[\"Lineage\"].agg(f).to_frame())\n",
        "\n",
        "# with open(FILELOC + 'covid_patient_seqs_20220228/covid_patient_seqs_grouped_20220228.pkl', 'wb') as f:\n",
        "#     pickle.dump(df_out, f)\n",
        "\n",
        "with open(FILELOC + 'covid_patient_seqs_20220228/covid_patient_seqs_grouped_20220228.pkl', 'rb') as f:\n",
        "    datadf = pickle.load(f)\n",
        "datadf.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1axurB8Ybo8f"
      },
      "source": [
        "The following is used to assign labels based on lineage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CiP_6BUNw6q"
      },
      "outputs": [],
      "source": [
        "# rename column with sequences to \"Spike\" for consistency with downstream operations\n",
        "datadf.rename(columns={'MaskedSeq':'Spike'}, inplace=True)\n",
        "\n",
        "datadf = datadf[datadf.Count >= 2].reset_index(drop=True)\n",
        "\n",
        "labelmap = {'AY.4':0, 'B.1.617.2':0,\n",
        "            'B.1':1, 'B.1.177':1, 'B.1.1':1, 'B.1.2':1,\n",
        "            'BA.1':2,\n",
        "            'BA.1.1':3,\n",
        "            'BA.2':4,\n",
        "            'P.1':5,\n",
        "            'B.1.351':6,\n",
        "            'B.1.427':7, 'B.1.429':7,\n",
        "            }\n",
        "\n",
        "datadf['Label'] = datadf['Lineage'].map(labelmap)\n",
        "datadf = datadf[datadf.Label.notna()].reset_index(drop=True)\n",
        "datadf['Label'] = datadf['Label'].astype(int)\n",
        "\n",
        "# aligned sequence lengths are equally 1273\n",
        "SEQLEN = 1273"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D18tKZKOjAm2"
      },
      "source": [
        "Tokenize sequences and create training and test data sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH8dQErijAm3"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(dataset, 'Spike', SEQLEN)\n",
        "\n",
        "# the code below defines the labels as genus label\n",
        "# can be modified to define labels as host label, or as both\n",
        "y = dataset.genus.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcXTabMCjAm3"
      },
      "outputs": [],
      "source": [
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.8*len(seqtok)), replace=False)\n",
        "# testindex = np.array([k for k in range(len(seqtok)) if k not in trainindex])\n",
        "\n",
        "# save train index for future use:\n",
        "# np.savetxt(FILELOC + 'corona_trainindex.csv', trainindex, fmt='%i', delimiter=',')\n",
        "trainindex = np.loadtxt(FILELOC + 'corona_trainindex.csv', dtype=int, delimiter=',')\n",
        "\n",
        "xtrain = seqtok[trainindex]\n",
        "xtest = seqtok[testindex];\n",
        "NVARS = 0       # there are no additional variables besides the sequence\n",
        "\n",
        "ytrain = y[trainindex]\n",
        "ytest = y[testindex]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JCe5_O_xIb4"
      },
      "source": [
        "##COVID-19 Disease Severity Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XUK6KX8XuAE"
      },
      "source": [
        "###Load up-to-date processed sequence dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkUMwl358gQw"
      },
      "outputs": [],
      "source": [
        "with open(FILELOC + 'covid_patient_seqs_20220415/covid_patient_20220415.pkl', 'rb') as f:\n",
        "    pdf = pickle.load(f)\n",
        "\n",
        "pdf.drop(columns=\"Unnamed: 0\", inplace=True)\n",
        "pdf['Lineage'] = pdf.Lineage.fillna('None')\n",
        "print(len(pdf))\n",
        "pdf.rename(columns={'reldate':'Date'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYrkUEYq4ujM"
      },
      "outputs": [],
      "source": [
        "# Use aligned sequences rather than \"raw\" sequences:\n",
        "\n",
        "pdf.drop(columns = 'Spike', inplace=True)\n",
        "\n",
        "# rename column with sequences to \"Spike\" for consistency with downstream operations\n",
        "pdf.rename(columns={'MaskedSeq':'Spike'}, inplace=True)\n",
        "# aligned sequence lengths are equally 1273\n",
        "SEQLEN = 1273"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8s70ojaXz0c"
      },
      "source": [
        "Use the following code to generate/regenerate labels (commented out because the csv file loaded above already has labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQq0Y2mxBTVH"
      },
      "outputs": [],
      "source": [
        "# labelmap = {'alive' : -1,\n",
        "#             'asymptomatic' : 0,\n",
        "#             'dead' : 1,\n",
        "#             'hospitalized' : 1,\n",
        "#             'mild' : 0,\n",
        "#             'moderate' : 0,\n",
        "#             'released' : 1,\n",
        "#             'screening' : 0,\n",
        "#             'severe' : 1,\n",
        "#             'symptomatic' : -1,\n",
        "#             'unknown' : -1,\n",
        "#         }\n",
        "\n",
        "# pdf['Label'] = pdf['Category'].map(labelmap).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WERzVmUX6K9"
      },
      "source": [
        "Remove samples with invalid labels and patient variables (which were set to -1 in preprocessing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVAmAugXYoHj"
      },
      "outputs": [],
      "source": [
        "# pdfbackup = pdf.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "di5bnxOD_u3a"
      },
      "outputs": [],
      "source": [
        "pdf.drop(pdf[pdf.Label==-1].index, inplace=True)\n",
        "print(len(pdf))\n",
        "pdf.drop(pdf[(pdf.Age==-1) | (pdf.Age > 100)].index, inplace=True)\n",
        "print(len(pdf))\n",
        "pdf.drop(pdf[pdf.Gender==-1].index, inplace=True)\n",
        "print(len(pdf))\n",
        "pdf.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xicXtrPP5pRt"
      },
      "source": [
        "Optionally clean the dataset of sequences with ambiguous or missing residues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-8TfQ5m57qB"
      },
      "outputs": [],
      "source": [
        "# pdfclean = pdf.drop(pdf[pdf['Spike'].str.contains('\\*|X')].index).reset_index(drop=True)\n",
        "# print(len(pdf), len(pdfclean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXJLpPnr59MU"
      },
      "source": [
        "Count sequence frequency (i.e. to exclude infrequent sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTrHLhRV_u0b"
      },
      "outputs": [],
      "source": [
        "# count all sequences\n",
        "countdata = pdf.groupby([\"Spike\"]).size().to_frame().rename(columns={0:\"Count\"}).reset_index(drop=False)\n",
        "pdfcount = pd.merge(pdf,countdata,on='Spike')\n",
        "\n",
        "try:\n",
        "    # count only sequences without * or X\n",
        "    countdata_cl = pdfclean.groupby([\"Spike\"]).size().to_frame().rename(columns={0:\"Count\"}).reset_index(drop=False)\n",
        "    pdfcount_cl = pd.merge(pdfclean,countdata_cl,on='Spike')\n",
        "\n",
        "    for m in [1000, 200, 100, 50, 10,5,4,3,2,1]:\n",
        "        print(m,\n",
        "            len(countdata[countdata.Count >= m]), len(pdfcount[pdfcount.Count >= m]),\n",
        "            len(countdata_cl[countdata_cl.Count >= m]), len(pdfcount_cl[pdfcount_cl.Count >= m]))\n",
        "except:\n",
        "    for m in [1000, 200, 100, 50, 10,5,4,3,2,1]:\n",
        "        print(m, len(countdata[countdata.Count >= m]), len(pdfcount[pdfcount.Count >= m]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_UZtZe0KJ4u"
      },
      "source": [
        "Optionally create a dataset of distinct sequences with averages of label and other variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDXTf0bBKOyp"
      },
      "outputs": [],
      "source": [
        "# pdfcount = pdf.groupby([\"Spike\"]).mean()\n",
        "# pdfcount['Count'] = pdf.groupby([\"Spike\"]).size()\n",
        "# def f(x):\n",
        "#     try:\n",
        "#         return pd.Series.mode(x)[0]\n",
        "#     except:\n",
        "#         return 'None'\n",
        "# c = pdf.groupby(\"Spike\")[\"Lineage\"].agg(f)\n",
        "# pdfcount = pdfcount.join(c)\n",
        "# pdfcount.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# pdfcount_cl = pdfcount.drop(pdfcount[pdfcount['Spike'].str.contains('\\*|X')].index).reset_index(drop=True)\n",
        "# print(len(pdfcount), len(pdfcount_cl))\n",
        "\n",
        "# for m in [1000, 200, 100, 50, 10,5,4,3,2,1]:\n",
        "#     print(m, len(pdfcount[pdfcount.Count >= m]), len(pdfcount_cl[pdfcount_cl.Count >= m]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8233rEGNfkZa"
      },
      "source": [
        "Predefined train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atePpI_jdyOT"
      },
      "outputs": [],
      "source": [
        "# traindf = pdfcount[pdfcount.Count >= 10]\n",
        "# testdf = pdfcount[pdfcount.Count < 10]\n",
        "# print(len(traindf), len(testdf))\n",
        "\n",
        "# # NVARS = 3       # 3 patient variables (age/gender/date)\n",
        "# # NCLASSES = 2    # 2 outcomes (mild/severe)\n",
        "# # trainvars = traindf[['Age', 'Gender', 'reldate']].values\n",
        "# # testvars = testdf[['Age', 'Gender', 'reldate']].values\n",
        "\n",
        "# NVARS = 2       # 3 patient variables (age/date)\n",
        "# NCLASSES = 2    # 2 outcomes (mild/severe)\n",
        "# trainvars = traindf[['Age', 'reldate']].values\n",
        "# testvars = testdf[['Age', 'reldate']].values\n",
        "\n",
        "# SEQLEN = 1273\n",
        "\n",
        "# trainseq = tokenize_sequences(traindf, 'Spike', SEQLEN)\n",
        "# testseq = tokenize_sequences(testdf, 'Spike', SEQLEN)\n",
        "\n",
        "# xtrain = [trainseq, trainvars]\n",
        "# xtest = [testseq, testvars]\n",
        "# ytrain = traindf.Label.values\n",
        "# ytest = testdf.Label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAw7NufYfjf_"
      },
      "source": [
        "Train and test sets determined through random split or lineage-based split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCJP2V-WXoB4"
      },
      "outputs": [],
      "source": [
        "# # use all sequences\n",
        "# tdf = pdf\n",
        "tdf = pdfcount\n",
        "# # use only \"clean\" sequences (no * or X)\n",
        "# tdf = pdfclean\n",
        "# tdf = pdfcount_cl\n",
        "# use only frequent sequences\n",
        "# tdf = pdfcount[pdfcount.Count >= 5]\n",
        "# # use only frequent sequences without * or X\n",
        "# tdf = pdfcount_cl[pdfcount_cl.Count >= 10]\n",
        "\n",
        "tdf = tdf.reset_index(drop=True)\n",
        "\n",
        "seqtok = tokenize_sequences(tdf, 'Spike', SEQLEN)\n",
        "y = tdf.Label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW3UbNOgBzY6"
      },
      "outputs": [],
      "source": [
        "# RANDOM SPLIT\n",
        "\n",
        "# trainindex = np.random.choice(range(len(seqtok)), size = int(0.7*len(seqtok)), replace=False)\n",
        "# testindex = np.array([k for k in range(len(seqtok)) if k not in trainindex])\n",
        "\n",
        "# save train index for future use:\n",
        "# np.savetxt(FILELOC + 'covidpatient_trainindex.csv', trainindex, fmt='%i', delimiter=',')\n",
        "\n",
        "# load predefined set of training indices\n",
        "# trainindex = np.loadtxt(FILELOC + 'covidpatient_trainindex.csv', dtype=int, delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDw0-598BUu_"
      },
      "outputs": [],
      "source": [
        "# LINEAGE SPLIT\n",
        "\n",
        "# trainindex = tdf[~tdf.Lineage.str.contains('BA')].index\n",
        "# testindex = tdf[tdf.Lineage.str.contains('BA')].index\n",
        "# trainindex = tdf[~tdf.Lineage.str.contains('BA.1.|BA.2.')].index\n",
        "# testindex = tdf[tdf.Lineage.str.contains('BA.1.|BA.2.')].index\n",
        "# trainindex = tdf[~(tdf.Lineage.str.contains('AY')|tdf.Lineage.str.contains('B.1.617.2'))].index\n",
        "# testindex = tdf[tdf.Lineage.str.contains('AY')|tdf.Lineage.str.contains('B.1.617.2')].index\n",
        "trainindex = tdf[~(tdf.Lineage.str.contains('AY'))].index\n",
        "testindex = tdf[tdf.Lineage.str.contains('AY')].index\n",
        "\n",
        "print(len(trainindex), len(testindex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0ss4SOm5Mgi"
      },
      "outputs": [],
      "source": [
        "# Use normalized date and ages instead\n",
        "\n",
        "# normalize date to 1000 max. and age to 100 max.\n",
        "tdf['reldate'] = tdf['Date'].apply(lambda x: x/1000)\n",
        "tdf['relage'] = tdf['Age'].apply(lambda x: x/100)\n",
        "\n",
        "# # otherwise uncomment below to use max. date and age to normalize\n",
        "# maxdate = pdf.Date.max()\n",
        "# pdf['reldate'] = pdf['Date'].apply(lambda x: x/maxdate)\n",
        "# maxage = pdf.Age.max()\n",
        "# pdf['relage'] = pdf['Age'].apply(lambda x: x/maxage)\n",
        "# print(maxdate, maxage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV8Lubex6u5p"
      },
      "outputs": [],
      "source": [
        "NCLASSES = 2    # 2 outcomes (mild/severe)\n",
        "\n",
        "# # Uncomment to do 3 variables including sample collection date\n",
        "\n",
        "# NVARS = 3       # 3 patient variables (age/gender/date)\n",
        "# vars = tdf[['relage', 'Gender', 'reldate']].values\n",
        "\n",
        "# # Uncomment if not using patient/sample-specific features (i.e., age, gender, date, etc.)\n",
        "\n",
        "# NVARS = 0\n",
        "\n",
        "# # Uncomment to use age and gender and not date\n",
        "\n",
        "NVARS = 2\n",
        "vars = tdf[['relage', 'Gender']].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wugkaD2x1gFR"
      },
      "source": [
        "Use regression to predict a continuous label between 0 to 1 vs. a binary classification of 0-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycNALcCC1dVJ"
      },
      "outputs": [],
      "source": [
        "# REGRESS = True\n",
        "REGRESS = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gg0mJ51IhFzf"
      },
      "outputs": [],
      "source": [
        "if NVARS == 0:\n",
        "    xtrain = seqtok[trainindex]\n",
        "    xtest = seqtok[testindex]\n",
        "else:\n",
        "    xtrain = [seqtok[trainindex], vars[trainindex]]\n",
        "    xtest = [seqtok[testindex], vars[testindex]]\n",
        "ytrain = y[trainindex]\n",
        "ytest = y[testindex]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOz1RBBWXolv"
      },
      "source": [
        "###Use train/test files that were previously used for earlier versions of the manuscript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANnzIKAw1Rpv"
      },
      "source": [
        "Sokhansanj, BA et al., \"An Interpretable Deep Learning Model for Predicting the Risk of Severe COVID-19 from Spike Protein Sequence\", https://www.researchsquare.com/article/rs-1234007/v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBi3VwKWArwc"
      },
      "outputs": [],
      "source": [
        "# # The file loaded below already has training and test sets defined\n",
        "\n",
        "# with open (FILELOC + 'covid_patient_seqs_20220228/covid_patient_data_old.pkl', 'rb') as f:\n",
        "#     traindf, testdf = pickle.load(f)\n",
        "# print(len(traindf), len(testdf))\n",
        "\n",
        "# # rename column with sequences to \"Spike\" for consistency\n",
        "# traindf.rename(columns={'ISM':'Spike'}, inplace=True)\n",
        "# testdf.rename(columns={'ISM':'Spike'}, inplace=True)\n",
        "\n",
        "# # NVARS = 3       # 3 patient variables (age/gender/date)\n",
        "# # NCLASSES = 2    # 2 outcomes (mild/severe)\n",
        "# # trainvars = traindf[['Age', 'Gender', 'reldate']].values\n",
        "# # testvars = testdf[['Age', 'Gender', 'reldate']].values\n",
        "\n",
        "# NVARS = 2       # 3 patient variables (age/date)\n",
        "# NCLASSES = 2    # 2 outcomes (mild/severe)\n",
        "# trainvars = traindf[['Age', 'reldate']].values\n",
        "# testvars = testdf[['Age', 'reldate']].values\n",
        "\n",
        "# SEQLEN = 1273"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgX-vGT2jDzN"
      },
      "source": [
        "Tokenize sequences and define \"xtrain/xtest/ytrain/ytest\" consistently with other methods for use in training and validation below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIskmxP2BpiD"
      },
      "outputs": [],
      "source": [
        "# trainseq = tokenize_sequences(traindf, 'Spike', SEQLEN)\n",
        "# testseq = tokenize_sequences(testdf, 'Spike', SEQLEN)\n",
        "\n",
        "# xtrain = [trainseq, trainvars]\n",
        "# xtest = [testseq, testvars]\n",
        "# ytrain = traindf.Label.values\n",
        "# ytest = testdf.Label.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiRleV1h3QHw"
      },
      "source": [
        "#Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naBxTAsNFVx1"
      },
      "source": [
        "##Class Balancing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St8XMfWCBucE"
      },
      "source": [
        "Class weights or sample weights can be used in the training code (it can be commented or commented out as needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2D1JqUZgsxf"
      },
      "outputs": [],
      "source": [
        "if not REGRESS:\n",
        "    from sklearn.utils import class_weight\n",
        "    class_weights = list(class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                        classes=np.arange(NCLASSES), y=ytrain))\n",
        "\n",
        "    # optionally, each sample may be weighted individually\n",
        "    sample_weights = np.array([class_weights[int(yi)] for yi in ytrain])\n",
        "    print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhOtMFUdCMMN"
      },
      "source": [
        "##Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6uGRVLxQ8kw"
      },
      "source": [
        "This training routine operates in one shot with a preset number of epochs. Early stopping is optional."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set REGRESS to False for classification and REGRESS to True for regression."
      ],
      "metadata": {
        "id": "nGtMUDcEVDkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97nazw7x4XLn"
      },
      "outputs": [],
      "source": [
        "# define # of epochs and batch sizes for a binary classification or\n",
        "# regression from 0-1 task (i.e., disease severity prediction)\n",
        "if not REGRESS:\n",
        "    NUM_EPOCHS = 50 # 70\n",
        "    # A larger batch size more optimally uses TPU resources\n",
        "    if tpu_env:\n",
        "        BATCH_SIZE = 48*8\n",
        "    else:\n",
        "        BATCH_SIZE = 48\n",
        "else:\n",
        "    # Use a smaller batch size on a smaller dataset & more epochs for regression\n",
        "    BATCH_SIZE = 48*8 # 48*8\n",
        "    NUM_EPOCHS = 250 # 100\n",
        "\n",
        "# # if doing multiclass prediction (i.e. for taxonomic classification)\n",
        "# NUM_EPOCHS = 25\n",
        "# BATCH_SIZE = 48     # there are not many training samples\n",
        "\n",
        "# See Tensorflow documentation for how to modify the early stopping callback\n",
        "\n",
        "# VAL_SPLIT = 0.2   # use if defining early stopping callbacks with validation data\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'loss',\n",
        "    verbose = 1,\n",
        "    patience = 10, #5,\n",
        "    mode = 'auto',\n",
        "    min_delta = 0,\n",
        "    restore_best_weights = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XkoitvF5IxJ"
      },
      "source": [
        "The below training code is used for a single training set and for a binary classification or regression from 0-1 (i.e., disease severity prediction tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2qNvsD6U7Ej5"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()    # reset tensorflow session (clear model history)\n",
        "\n",
        "if tpu_env:\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model(regress=REGRESS, singleclass=(not REGRESS), multiclass=False,\n",
        "                            output_multiheadatt=True, use_att=True, nclasses=NCLASSES,\n",
        "                            output_two=False, numvars=NVARS, mask=True)\n",
        "        model.summary() # Output table of deep model layers and parameters / connections\n",
        "else:\n",
        "    model = reset_model(regress=REGRESS, singleclass=(not REGRESS), multiclass=True,\n",
        "                        output_multiheadatt=False, use_att=True, nclasses=NCLASSES,\n",
        "                        numvars=NVARS, mask=False)\n",
        "    model.summary() # Output table of deep model layers and parameters / connections\n",
        "\n",
        "history = model.fit(xtrain, ytrain,\n",
        "                # optional sample weighting\n",
        "                # sample_weight = sample_weights,\n",
        "                # use the following instead of sample_weight if weighting samples individually\n",
        "                # class_weight = {c:class_weights[c] for c in range(NCLASSES)},\n",
        "                batch_size = BATCH_SIZE,\n",
        "                epochs = NUM_EPOCHS,\n",
        "                verbose = 1,\n",
        "                # validation_split = VAL_SPLIT,\n",
        "                callbacks = [early_stopping],\n",
        "                )\n",
        "\n",
        "# optionally save weights\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2.h5\", save_format='h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y04OICR2RDa_"
      },
      "source": [
        "Output confusion matrix and sklearn classification report (precision, recall, f1-score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yefzQysZry0l"
      },
      "outputs": [],
      "source": [
        "print(\"Results for Testing Data:\")\n",
        "test_predict = model.predict(xtest)\n",
        "ClassRep = classification_report(np.round(ytest), np.round(test_predict))\n",
        "ConfMatrix = confusion_matrix(np.round(ytest), np.round(test_predict))\n",
        "print(ClassRep)\n",
        "print(ConfMatrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pNLi_rCq8yaY"
      },
      "outputs": [],
      "source": [
        "# sample model save\n",
        "\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2_test_Delta_AY_agegender_2.h5\", save_format='h5', overwrite=True)\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2_test_Omicron_subvar_agegender.h5\", save_format='h5', overwrite=True)\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2_test_Omicron.h5\", save_format='h5', overwrite=True)\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2_test_Omicron_subvar.h5\", save_format='h5', overwrite=True)\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2_test_Omicron_subvar_nodemo.h5\", save_format='h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvA3_D_v8Kho"
      },
      "outputs": [],
      "source": [
        "# Continue training for additional epochs\n",
        "\n",
        "EXTRA_EPOCHS = 20\n",
        "\n",
        "history = model.fit(xtrain, ytrain,\n",
        "                    # sample_weight = sample_weights,\n",
        "                    # use the following instead of sample_weight if weighting samples individually\n",
        "                    # class_weight = {c:class_weights[c] for c in range(NCLASSES)},\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    epochs = EXTRA_EPOCHS,\n",
        "                    verbose = 1,\n",
        "                    # validation_split = VAL_SPLIT,\n",
        "                    # callbacks = [early_stopping],\n",
        "                    )\n",
        "\n",
        "# model.save_weights(f\"{FILELOC}weights_sarscov2_extra_epochs.h5\", save_format='h5', overwrite=True)\n",
        "\n",
        "print(\"Results for Testing Data:\")\n",
        "test_predict = model.predict(xtest)\n",
        "ClassRep = classification_report(np.round(ytest), np.round(test_predict))\n",
        "ConfMatrix = confusion_matrix(np.round(ytest), np.round(test_predict))\n",
        "print(ClassRep)\n",
        "print(ConfMatrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1tttz3uSbpV"
      },
      "outputs": [],
      "source": [
        "# example of doing MULTIPLE REPEAT RUNS\n",
        "# assume tpu_env is True\n",
        "\n",
        "for run in range(3):\n",
        "    tf.keras.backend.clear_session()    # reset tensorflow session (clear model history)\n",
        "\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model(regress=REGRESS, singleclass=(not REGRESS), multiclass=False,\n",
        "                            output_multiheadatt=True, use_att=True, nclasses=NCLASSES,\n",
        "                            output_two=False, numvars=NVARS, mask=True)\n",
        "\n",
        "        history = model.fit(xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose = 1)\n",
        "        model.save_weights(f\"{FILELOC}weights_sarscov2_test_AY_{run}.h5\")\n",
        "        test_predict = model.predict(xtest)\n",
        "        print(classification_report(np.round(ytest), np.round(test_predict)))\n",
        "        print(confusion_matrix(np.round(ytest), np.round(test_predict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAerrFVqkKwc"
      },
      "source": [
        "##Load Pretrained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU06p34bkKwd"
      },
      "source": [
        "The arguments for reset_model() should be the same as used for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx1n-pOskKwd"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "if tpu_env:\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model(regress=False, singleclass=True, multiclass=False,\n",
        "                            output_multiheadatt=True, use_att=True, nclasses=NCLASSES,\n",
        "                            output_two=False, numvars=NVARS, mask=True)\n",
        "        model.load_weights(f\"{FILELOC}paper_resubmit/weights_sarscov2_test_Delta.h5\")\n",
        "        model.compile()\n",
        "else:\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    model.load_weights(f\"{FILELOC}weights_sarscov2_20220408.h5\")\n",
        "    model.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkr44-7Tv0wI"
      },
      "source": [
        "Load multiple runs and print out classification reports/confusion matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcbQnofnv0Ka"
      },
      "outputs": [],
      "source": [
        "for run in range(5):\n",
        "    tf.keras.backend.clear_session()\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model(regress=False, singleclass=True, multiclass=False,\n",
        "                            output_multiheadatt=True, use_att=True, nclasses=NCLASSES,\n",
        "                            output_two=False, numvars=NVARS, mask=True)\n",
        "        model.load_weights(f\"{FILELOC}weights_sarscov2_test_Omicron_subvar_agegender_{run}.h5\")\n",
        "        model.compile()\n",
        "        test_predict = model.predict(xtest, batch_size=256, verbose=False)\n",
        "        print(f\"RUN= {run}\")\n",
        "        print(classification_report(np.round(ytest), np.round(test_predict)))\n",
        "        print(confusion_matrix(np.round(ytest), np.round(test_predict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Attention and Embedding values"
      ],
      "metadata": {
        "id": "r1Ez5mMDdPul"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAYtWu--zcKM"
      },
      "outputs": [],
      "source": [
        "# # this cell is an example of loading two different sets of attention and embeddings\n",
        "\n",
        "# NVARS = 2; xtrain = [seqtok[trainindex], vars[trainindex]]; xtest = [seqtok[testindex], vars[testindex]]\n",
        "# tf.keras.backend.clear_session()\n",
        "# with tpu_strategy.scope():\n",
        "#     model = reset_model(regress=False, singleclass=True, multiclass=False,\n",
        "#                             output_multiheadatt=True, use_att=True, nclasses=NCLASSES,\n",
        "#                             output_two=False, numvars=NVARS, mask=True)\n",
        "#     model.load_weights(f\"{FILELOC}weights_sarscov2_test_Omicron_subvar_agegender_4.h5\")\n",
        "#     get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "#     get_embedding_model.compile()\n",
        "#     get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "#     get_attention_model.compile()\n",
        "#     pred = model.predict([seqtok,vars], verbose=True)\n",
        "#     emb = get_embedding_model.predict([seqtok,vars], verbose=True)\n",
        "#     att = get_attention_model.predict([seqtok,vars], verbose=True)\n",
        "# with open(FILELOC + 'paper_resubmit/transformer_Omicron_subvar_agegender_eval.pkl', 'wb') as f:\n",
        "#     pickle.dump([pred, emb, att], f)\n",
        "\n",
        "# NVARS = 0; xtrain = seqtok[trainindex]; xtest = seqtok[testindex]\n",
        "# tf.keras.backend.clear_session()\n",
        "# with tpu_strategy.scope():\n",
        "#     model = reset_model(regress=False, singleclass=True, multiclass=False,\n",
        "#                             output_multiheadatt=True, use_att=True, nclasses=NCLASSES,\n",
        "#                             output_two=False, numvars=NVARS, mask=True)\n",
        "#     model.load_weights(f\"{FILELOC}weights_sarscov2_test_Omicron_subvar_nodemo_0.h5\")\n",
        "#     get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "#     get_embedding_model.compile()\n",
        "#     get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "#     get_attention_model.compile()\n",
        "#     pred = model.predict(seqtok, verbose=True)\n",
        "#     emb = get_embedding_model.predict(seqtok, verbose=True)\n",
        "#     att = get_attention_model.predict(seqtok, verbose=True)\n",
        "# with open(FILELOC + 'paper_resubmit/transformer_Omicron_subvar_nodemo_eval.pkl', 'wb') as f:\n",
        "#     pickle.dump([pred, emb, att], f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGSWxHXDfE6-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5iQD5bVSZ8e"
      },
      "source": [
        "###Plot Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHxwNQngdQGi"
      },
      "outputs": [],
      "source": [
        "with open(FILELOC + 'paper_resubmit/transformer_Omicron_subvar_agegender_eval.pkl', 'rb') as f:\n",
        "    _, _, att_var = pickle.load(f)\n",
        "with open(FILELOC + 'paper_resubmit/transformer_Omicron_subvar_nodemo_eval.pkl', 'rb') as f:\n",
        "    _, _, att_novar = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3UgDmKEjAmI"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots()\n",
        "fig.set_size_inches(16,6)\n",
        "\n",
        "ax.plot(range(1,SEQLEN+1), np.median(att_var,axis=0), linewidth=1, label=\"Age/Gender\", color='red')\n",
        "ax.plot(range(1,SEQLEN+1), np.median(att_novar,axis=0), linewidth=1, label=\"Sequence Only\", color='blue')\n",
        "\n",
        "ax.set_xlabel('Sequence Position', fontsize=18, fontweight='bold')\n",
        "ax.set_ylabel('Attention', fontsize=18, fontweight='bold')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXW13K32jAi-"
      },
      "outputs": [],
      "source": [
        "# output the specific locations of high attention\n",
        "\n",
        "np.where(np.median(att_var,axis=0) > 0.001)[0]+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSkkSpKjSb-u"
      },
      "source": [
        "###Plot Multi-Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdBfpPd7lH0A"
      },
      "source": [
        "Sample code to plot and extract attention from transformer heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oXnF3fkYfji"
      },
      "outputs": [],
      "source": [
        "trainseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgYg9w3-VyjK"
      },
      "outputs": [],
      "source": [
        "output,wts = get_mha_model.predict([seqtok[topdelta],vars[topdelta]], verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4t5fnLEV6hh"
      },
      "outputs": [],
      "source": [
        "tdf.loc[topdelta].index.get_loc(13333)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-8TtLLIhJr4"
      },
      "outputs": [],
      "source": [
        "# output,wts = get_mha_model.predict([trainseq[0:2],trainvars[0:2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5K0fqmDwfUg"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import LogNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHzouzRSYzVq"
      },
      "outputs": [],
      "source": [
        "k = 22\n",
        "\n",
        "norm = {}\n",
        "for m in range(4):\n",
        "    vmin = np.min(np.min(wts[k,m]))\n",
        "    vmax=np.max(np.max(wts[k,m]))\n",
        "    norm[m] = LogNorm(vmin=vmin, vmax=vmax)\n",
        "    print(vmin, vmax)\n",
        "\n",
        "fig,ax = plt.subplots(2,2)\n",
        "fig = fig.set_size_inches(18,18)\n",
        "a = [ax[0,0], ax[1,0], ax[0,1], ax[1,1]]\n",
        "for m in range(4):\n",
        "    try:\n",
        "        sns.heatmap(wts[k,m], ax = a[m], norm = norm[m])\n",
        "    except:\n",
        "        pass\n",
        "# sns.heatmap(wts[k,0]/np.median(np.median(wts[k,0])), ax=ax[0,0])\n",
        "# sns.heatmap(wts[k,1]/np.median(np.median(wts[k,1])), ax=ax[1,0])\n",
        "# sns.heatmap(wts[k,2]/np.median(np.median(wts[k,2])), ax=ax[0,1])\n",
        "# sns.heatmap(wts[k,3]/np.median(np.median(wts[k,3])), ax=ax[1,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2uY_4PxSiGh"
      },
      "outputs": [],
      "source": [
        "# k = 0\n",
        "\n",
        "# norm = {}\n",
        "# for m in range(8):\n",
        "#     norm[m] = LogNorm(vmin=np.min(np.min(wts[k,m])), vmax=np.max(np.max(wts[k,m])))\n",
        "\n",
        "# fig,ax = plt.subplots(4,2)\n",
        "# fig = fig.set_size_inches(18,18)\n",
        "# a = [ax[0,0], ax[1,0], ax[2,0], ax[3,0],\n",
        "#      ax[0,1], ax[1,1], ax[2,1], ax[3,1]]\n",
        "# for m in range(8):\n",
        "#     sns.heatmap(wts[k,m], ax = a[m], norm = norm[m])\n",
        "\n",
        "# # sns.heatmap(wts[k,0]/np.median(np.median(wts[k,0])), ax=ax[0,0])\n",
        "# # sns.heatmap(wts[k,1]/np.median(np.median(wts[k,1])), ax=ax[1,0])\n",
        "# # sns.heatmap(wts[k,2]/np.median(np.median(wts[k,2])), ax=ax[2,0])\n",
        "# # sns.heatmap(wts[k,3]/np.median(np.median(wts[k,3])), ax=ax[3,0])\n",
        "# # sns.heatmap(wts[k,4]/np.median(np.median(wts[k,4])), ax=ax[0,1])\n",
        "# # sns.heatmap(wts[k,5]/np.median(np.median(wts[k,5])), ax=ax[1,1])\n",
        "# # sns.heatmap(wts[k,6]/np.median(np.median(wts[k,6])), ax=ax[2,1])\n",
        "# # sns.heatmap(wts[k,7]/np.median(np.median(wts[k,7])), ax=ax[3,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7_OLGmMA9Mq"
      },
      "outputs": [],
      "source": [
        "np.where(np.median(att, axis=0) > 1/1273)[0]+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9IE3wsxAyZJ"
      },
      "outputs": [],
      "source": [
        "mha = {}\n",
        "for h in range(NHEADS):\n",
        "    mha[h] = np.sum(wts[0,h], axis=0)\n",
        "mhadf = pd.DataFrame.from_dict({f'Head {h}':mha[h] for h in range(NHEADS)})\n",
        "topdf = pd.DataFrame.from_dict({f'Head {h}':mhadf.sort_values(by=f'Head {h}', ascending=False).head(50).index + 1 for h in range(NHEADS)})\n",
        "display(topdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3R5TOtQzPr_"
      },
      "outputs": [],
      "source": [
        "# mha = {}\n",
        "# for h in range(8):\n",
        "#     mha[h] = np.sum(wts[0,h], axis=0)\n",
        "# mhadf = pd.DataFrame.from_dict({f'Head {h}':mha[h] for h in range(8)})\n",
        "# topdf = pd.DataFrame.from_dict({f'Head {h}':mhadf.sort_values(by=f'Head {h}', ascending=False).head(20).index for h in range(8)})\n",
        "# display(topdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qstqdVorlhgn"
      },
      "source": [
        "##Plot Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o67yVGwvlfYp"
      },
      "source": [
        "Sample code to plot TSNE of embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR6Anne1en5K"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "t = TSNE(n_components=2).fit_transform(emb, perplexity=50, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F_mjEjlWsvv"
      },
      "outputs": [],
      "source": [
        "Lineages = ['A', 'B.1', 'B.1.1.7', 'B.1.351', 'P.1', 'B.1.617.2', 'AY.4', 'BA.1', 'BA.2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSL33soRW5V_"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches(12,8)\n",
        "\n",
        "for lin in Lineages:\n",
        "    selind = tdf[tdf.Lineage == lin].index\n",
        "    ax.scatter(t[selind,0], t[selind,1], marker='x', label=lin)\n",
        "\n",
        "ax.legend(bbox_to_anchor=(1.0, 0.9),  framealpha=1.0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyRfKZBZWkrv"
      },
      "outputs": [],
      "source": [
        "# fig, ax = plt.subplots()\n",
        "# fig.set_size_inches(12,8)\n",
        "\n",
        "# genus = ['Alphacoronavirus', 'Betacoronavirus', 'Gammacoronavirus', 'Deltacoronavirus']\n",
        "# for genus_ind in np.unique(ytest):\n",
        "#     selind = np.where(ytest==genus_ind)[0]\n",
        "#     ax.scatter(t[selind,0], t[selind,1], marker='x', label=genus[genus_ind])\n",
        "# ax.scatter(t[len(ytest):,0], t[len(ytest):,1], marker='x', label='Omicron')\n",
        "\n",
        "# ax.legend(bbox_to_anchor=(1.0, 0.9),  framealpha=1.0)\n",
        "# plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Corona_Transformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1hoyviiHrBiBrzq34P4ytxC9OEf0ao9Dw",
      "authorship_tag": "ABX9TyNqaG4Yknb4espJtQN7Txp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}