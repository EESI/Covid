{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid_Deep_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1nxzD1atzba5pxRhbeCs0gNB0spnhUWJh",
      "authorship_tag": "ABX9TyN5ruKjQAX66cIGbBh4HKmw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahrad/Covid/blob/main/Covid_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzkSry5x4VaL"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeO8oplg45X-"
      },
      "source": [
        "Minimal imports (Tensorflow singularity packages + Pandas for output)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLa737hB7e4_"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiNZfPTu0_em",
        "outputId": "a6e29682-ea32-48ec-d54a-55d81570d467"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILELOC = \"/content/drive/My Drive/COVID_Python/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INmsA86dCdMV",
        "outputId": "f15032c4-3fdd-4c64-c6cb-f5f6e41cb2a6"
      },
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    tpu_env=True\n",
        "except ValueError:\n",
        "    print('Not connected to a TPU runtime.')\n",
        "    tpu_env=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a TPU runtime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDh49-3d425W"
      },
      "source": [
        "Data Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4YNsZB38MS8"
      },
      "source": [
        "DATA_FILE= FILELOC + 'covid_aligned_crossval/TrainData.csv'\n",
        "LABEL_FILE= FILELOC + 'covid_aligned_crossval/TrainLabels_Age.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx20W_hY400m"
      },
      "source": [
        "Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsAfrYxP75be"
      },
      "source": [
        "ENCDIM = 1500\n",
        "NC = 300\n",
        "NL = 1\n",
        "NT = 1\n",
        "NHEADS = 8\n",
        "FFDIM = 64\n",
        "NDENSE = 64\n",
        "TRANSDROPRATE = 0.1\n",
        "DROPRATE = 0.0\n",
        "\n",
        "LEARN_RATE = 0.0001\n",
        "\n",
        "BATCH_SIZE = 48\n",
        "\n",
        "STEPS_PER_EXECUTION = 50\n",
        "\n",
        "ismlen = 1273"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EfE13Hp8AJ9"
      },
      "source": [
        "def reset_model():\n",
        "\n",
        "    model = AttModel(L=ismlen,\n",
        "                     vocab_size=len(aa_list)+1,\n",
        "                     embdim = ENCDIM,\n",
        "                     numheads = NHEADS,\n",
        "                     ffdim = FFDIM,\n",
        "                     num_dense = NDENSE,\n",
        "                     mask_zero=True,\n",
        "                     dropout_rate = DROPRATE,\n",
        "                     trans_drop = TRANSDROPRATE,\n",
        "                     Nt = NT,\n",
        "                     W = 1, Nc = NC, Nl = NL,\n",
        "                     )\n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARN_RATE)\n",
        "\n",
        "\n",
        "    loss = keras.losses.MeanSquaredError()\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARN_RATE)\n",
        "    metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "               keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "               keras.losses.MeanAbsoluteError(name='mae')\n",
        "               ]\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics,\n",
        "                  steps_per_execution = STEPS_PER_EXECUTION)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK05TjTo79ql"
      },
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, mask_zero=False):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                output_dim=embed_dim,\n",
        "                                                mask_zero=mask_zero)\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,\n",
        "                                              mask_zero=mask_zero)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "def linear01(x):\n",
        "    return tf.clip_by_value(x, clip_value_min=0, clip_value_max=1)\n",
        "\n",
        "def AttModel(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False):\n",
        "\n",
        "    inpTensor = keras.Input(shape=(L,))\n",
        "    x = inpTensor\n",
        "\n",
        "    if mask_zero:\n",
        "        x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x)\n",
        "\n",
        "    # Attention layer\n",
        "    h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "    attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "    attention = keras.layers.Flatten()(attention)  \n",
        "    attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "\n",
        "    # attention = keras.layers.RepeatVector(embdim)(attention)\n",
        "    attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "\n",
        "    attention = keras.layers.Permute([2, 1])(attention)\n",
        "    representation = keras.layers.multiply([h, attention])\n",
        "    representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "    x = representation\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(x)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    model = keras.Model(inpTensor,finalOut)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_A3wayA6595"
      },
      "source": [
        "labels = np.loadtxt(LABEL_FILE)\n",
        "with open(DATA_FILE, 'r') as f:\n",
        "    readfile = csv.reader(f)\n",
        "    a = []\n",
        "    for line in readfile:\n",
        "        if len(line) < 1273:\n",
        "            line += '*'*(1273-len(line))\n",
        "        elif len(line) > 1273:\n",
        "            line = line[:1273]\n",
        "        a.append(line)\n",
        "    ism_array = np.vstack(a)\n",
        "\n",
        "aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "          'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "          'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "          ]\n",
        "aa_tokenizer = {aa_list[k]:k+1 for k in range(len(aa_list))}\n",
        "aa_tokenizer['*'] = 0\n",
        "aa_tokenizer['X'] = 0\n",
        "x_train_tok = np.vectorize(aa_tokenizer.get)(ism_array)\n",
        "\n",
        "y_train = labels\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'loss',\n",
        "    verbose = 1,\n",
        "    patience = 20,\n",
        "    mode = 'auto',\n",
        "    min_delta = 0,\n",
        "    baseline = 0.2,\n",
        "    restore_best_weights = True\n",
        "    )\n",
        "\n",
        "early_stopping_2 = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'loss',\n",
        "    verbose = 1,\n",
        "    patience = 200,\n",
        "    mode = 'auto',\n",
        "    min_delta = 0,\n",
        "    baseline = 0.2,\n",
        "    restore_best_weights = True\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1PwHWUJ0pe2"
      },
      "source": [
        "# Fit Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUmJSAoYf0Hg"
      },
      "source": [
        "model = reset_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfLqnx8jf1EQ",
        "outputId": "f5f248af-1f3d-44e0-efea-f6424299d272"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1273)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masking (Masking)               (None, 1273)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "token_and_position_embedding (T (None, 1273, 1500)   1942500     masking[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 1273, 300)    450300      token_and_position_embedding[0][0\n",
            "__________________________________________________________________________________________________\n",
            "transformer_block (TransformerB (None, 1273, 300)    2927464     conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 1273, 300)    90300       transformer_block[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 1273, 1)      301         time_distributed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1273)         0           time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "attention (Softmax)             (None, 1273)         0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector (RepeatVector)    (None, 300, 1273)    0           attention[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "permute (Permute)               (None, 1273, 300)    0           repeat_vector[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 1273, 300)    0           time_distributed[0][0]           \n",
            "                                                                 permute[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_sum (TFOpLambda) (None, 300)          0           multiply[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64)           19264       tf.math.reduce_sum[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            65          dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,430,194\n",
            "Trainable params: 5,430,194\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiqOgPdv7HXh"
      },
      "source": [
        "n_epochs = 0\n",
        "while (n_epochs < 35):\n",
        "    tf.keras.backend.clear_session()\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model()\n",
        "    history = model.fit(x_train_tok,\n",
        "                        y_train,\n",
        "                        batch_size = BATCH_SIZE,\n",
        "                        epochs = 600,\n",
        "                        verbose = 2,\n",
        "                        callbacks = [early_stopping],\n",
        "                        )\n",
        "    n_epochs = len(history.history['loss'])\n",
        "\n",
        "if n_epochs < 250:\n",
        "        history = model.fit(x_train_tok,\n",
        "                        y_train,\n",
        "                        batch_size = BATCH_SIZE,\n",
        "                        epochs = 600,\n",
        "                        verbose = 2,\n",
        "                        callbacks = [early_stopping_2],\n",
        "                        )\n",
        "\n",
        "model.save_weights(FILELOC + 'covid_trans_fulldata_age_20211025_1.h5', save_format='h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1jHxOaK0sBQ"
      },
      "source": [
        "# Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YSEOpE42BRB"
      },
      "source": [
        "nruns = 5\n",
        "\n",
        "aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "          'P', 'S', 'T', 'W', 'Y', 'V', '-',]\n",
        "aa_tokenizer = {aa_list[k]:k+1 for k in range(len(aa_list))}\n",
        "aa_tokenizer['*'] = 0\n",
        "aa_tokenizer['X'] = 0\n",
        "def padder(x):\n",
        "    if len(x) < 1273:\n",
        "        return x + '*'*(1273-len(x))\n",
        "    elif len(x) > 1273:\n",
        "        return x[:1273]\n",
        "    else:\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96A0fvF15LMG"
      },
      "source": [
        "Predictions on validation data set + creation of attention and embedding outputs for training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiaNhwif0ZWh"
      },
      "source": [
        "# model_dir = FILELOC + 'covidout_trans_fulldata_age/'\n",
        "# traindata = pd.read_csv(FILELOC + 'covid_aligned_crossval/covid_patient_0912_aligned.csv')\n",
        "# valdata = pd.read_csv(FILELOC + 'covid_aligned_crossval/covid_patient_0912_valid1001_aligned.csv')\n",
        "model_dir = FILELOC + 'covidout_trans_fulldata_age_raw/'\n",
        "traindata = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_rawseqs.csv')\n",
        "valdata = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_valid1001_rawseqs.csv')\n",
        "\n",
        "ismlen = 1273\n",
        "\n",
        "train_array = np.vstack(traindata['ISM'].apply(padder).apply(lambda x: np.array(list(x))))\n",
        "val_array = np.vstack(valdata['ISM'].apply(padder).apply(lambda x: np.array(list(x))))\n",
        "train_tok = np.vectorize(aa_tokenizer.get)(train_array) \n",
        "val_tok = np.vectorize(aa_tokenizer.get)(val_array)\n",
        "y_train = traindata['Label'].values\n",
        "\n",
        "MSE = {}\n",
        "\n",
        "for run in range(1, nruns+1):\n",
        "    tf.keras.backend.clear_session()    # reset Tensorflow session\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model()       \n",
        "        # model.load_weights(model_dir + 'covid_trans_fulldata_age_20211025_'+str(run)+'.h5')\n",
        "        model.load_weights(model_dir + 'covid_rawseqs_fulldata_age_weights_20211025_'+str(run)+'.h5')\n",
        "\n",
        "        pred = model.predict(train_tok, verbose=2).ravel()\n",
        "        np.savetxt(model_dir + 'covid_traindata0912_pred_age_'+str(run)+'.csv', pred, delimiter=',')\n",
        "        traindata['Predict'] = pred\n",
        "        get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "        att = get_attention_model.predict(train_tok, verbose=2, batch_size=32)\n",
        "        np.savetxt(model_dir + 'covid_traindata0912_att_age_'+str(run)+'.csv', att, delimiter=',')\n",
        "\n",
        "        traindata['Attention'] = [a for a in att]\n",
        "        get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "        emb = get_embedding_model.predict(train_tok, verbose=2, batch_size=32)\n",
        "        np.savetxt(model_dir + 'covid_traindata0912_emb_age_'+str(run)+'.csv', emb, delimiter=',')\n",
        "\n",
        "        pred = model.predict(val_tok, verbose=2).ravel()\n",
        "        np.savetxt(model_dir + 'covid_valdata1001_pred_age_'+str(run)+'.csv', pred, delimiter=',')\n",
        "        get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "        att = get_attention_model.predict(val_tok, verbose=2, batch_size=32)\n",
        "        np.savetxt(model_dir + 'covid_valdata1001_att_age_'+str(run)+'.csv', att, delimiter=',')\n",
        "        get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "        emb = get_embedding_model.predict(val_tok, verbose=2, batch_size=32)\n",
        "        np.savetxt(model_dir + 'covid_valdata1001_emb_age_'+str(run)+'.csv', emb, delimiter=',')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_ifFA9N1ge1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyRs3dKx2Jzc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}