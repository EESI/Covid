{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bahrad/Covid/blob/main/Covid_Pred_Taxonomy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzkSry5x4VaL"
      },
      "source": [
        "#Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLa737hB7e4_"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTrJ7gQ5cG4F",
        "outputId": "ec78bbcd-7a88-4b15-97ef-07c31bd42c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiNZfPTu0_em"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "FILELOC = \"/content/drive/My Drive/COVID_Python/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INmsA86dCdMV",
        "outputId": "1b180c9e-f9ce-422f-db26-d0cb6844162b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.27.255.98:8470']\n",
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.27.255.98:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.27.255.98:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    tpu_env=True\n",
        "except ValueError:\n",
        "    print('Not connected to a TPU runtime.')\n",
        "    tpu_env=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EfE13Hp8AJ9"
      },
      "outputs": [],
      "source": [
        "def reset_model(regress, singleclass, multiclass, output_multiheadatt, use_att, nclasses=4,\n",
        "                output_two=False):\n",
        "\n",
        "    if output_multiheadatt:\n",
        "        model_fn = AttMod_2\n",
        "    elif output_two:\n",
        "        model_fn = AttMod_3\n",
        "    else:\n",
        "        model_fn = AttModel\n",
        "\n",
        "    model = model_fn(L=ismlen,\n",
        "                     vocab_size=len(aa_list)+1,\n",
        "                     embdim = ENCDIM,\n",
        "                     numheads = NHEADS,\n",
        "                     ffdim = FFDIM,\n",
        "                     num_dense = NDENSE,\n",
        "                     mask_zero=True,\n",
        "                     dropout_rate = DROPRATE,\n",
        "                     trans_drop = TRANSDROPRATE,\n",
        "                     Nt = NT,\n",
        "                     W = 1, Nc = NC, Nl = NL,\n",
        "                     regress=regress, singleclass=singleclass,\n",
        "                     multiclass=multiclass, use_att=use_att,\n",
        "                     nclasses=nclasses,\n",
        "                     )\n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARN_RATE)\n",
        "    if regress:\n",
        "        loss = keras.losses.MeanSquaredError()\n",
        "        metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "            keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "            keras.losses.MeanAbsoluteError(name='mae')\n",
        "            ]\n",
        "    if singleclass:\n",
        "        loss = keras.losses.BinaryCrossentropy()\n",
        "        metrics = [keras.metrics.BinaryAccuracy(name='acc'),\n",
        "                   keras.metrics.AUC(name='auc')]\n",
        "    if multiclass:    \n",
        "        loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "        metrics = [keras.metrics.SparseCategoricalAccuracy(name='acc')]\n",
        "\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics,)\n",
        "                #   steps_per_execution = STEPS_PER_EXECUTION,)\n",
        "\n",
        "    if output_two:\n",
        "        losses = {'outfirst':'mean_squared_error',\n",
        "                  'outpeak':'mean_squared_error'}\n",
        "        lossweights = {'outfirst':1.0, 'outpeak':1.0}\n",
        "        metrics = [keras.metrics.MeanSquaredError(name='mse'),\n",
        "                   keras.metrics.MeanSquaredLogarithmicError(name='msle'),\n",
        "                   keras.losses.MeanAbsoluteError(name='mae')]\n",
        "        model.compile(loss=losses, loss_weights=lossweights, optimizer=optimizer,metrics=metrics)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGYlfhd95NKk"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [keras.layers.Dense(ff_dim, activation=\"relu\"), keras.layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, mask_zero=False):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size,\n",
        "                                                output_dim=embed_dim,\n",
        "                                                mask_zero=mask_zero)\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim,\n",
        "                                              mask_zero=mask_zero)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "def linear01(x):\n",
        "    return tf.clip_by_value(x, clip_value_min=0, clip_value_max=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPCCClpo7Rim"
      },
      "outputs": [],
      "source": [
        "def AttMod_2(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4):\n",
        "\n",
        "    inpTensor = keras.Input(shape=(L,))\n",
        "    x = inpTensor\n",
        "\n",
        "    if mask_zero:\n",
        "        x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    y, attout = keras.layers.MultiHeadAttention(num_heads=numheads, key_dim=embdim,\n",
        "                                                )(x, x, return_attention_scores=True)\n",
        "    y = keras.layers.Dropout(trans_drop)(y)\n",
        "    z = keras.layers.LayerNormalization(epsilon=1e-6)(x + y)\n",
        "    z1 = keras.Sequential( [keras.layers.Dense(ffdim, activation=\"relu\"), keras.layers.Dense(embdim),])\n",
        "    z1 = keras.layers.Dropout(trans_drop)(z)\n",
        "    x = keras.layers.LayerNormalization(epsilon=1e-6)(z + z1)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(x)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    model = keras.Model(inpTensor,finalOut)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KK05TjTo79ql"
      },
      "outputs": [],
      "source": [
        "def AttModel(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4):\n",
        "\n",
        "    inpTensor = keras.Input(shape=(L,))\n",
        "    x = inpTensor\n",
        "\n",
        "    if mask_zero:\n",
        "        x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(x)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    # define the model's start and end points    \n",
        "    model = keras.Model(inpTensor,finalOut)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX-ogBerSt8P"
      },
      "outputs": [],
      "source": [
        "def AttMod_3(L, vocab_size, embdim, numheads, ffdim, num_dense=False,\n",
        "             mask_zero=False, dropout_rate=False, trans_drop=0.1,\n",
        "             Nt=1, W=False, Nc=False, Nl=False,\n",
        "             regress=True, singleclass=False, multiclass=False, use_att=True,\n",
        "             nclasses=4):\n",
        "\n",
        "    inpTensor = keras.Input(shape=(L,))\n",
        "    x = inpTensor\n",
        "\n",
        "    if mask_zero:\n",
        "        x = keras.layers.Masking(mask_value=0)(x)   \n",
        "\n",
        "    x = TokenAndPositionEmbedding(L, vocab_size, embdim, mask_zero)(x)\n",
        "\n",
        "    if W and Nc and Nl:\n",
        "        for n in range(Nl):\n",
        "            x = keras.layers.Conv1D(filters = Nc,\n",
        "                                kernel_size = W,\n",
        "                                activation = 'relu',\n",
        "                                padding = 'same',\n",
        "                                )(x)\n",
        "            if n > 1 and n < Nl-1:\n",
        "                x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    for n in range(Nt):\n",
        "        x = TransformerBlock(Nc, numheads, ffdim, rate=trans_drop)(x)\n",
        "\n",
        "    if use_att:\n",
        "        # Attention layer\n",
        "        h = keras.layers.TimeDistributed(keras.layers.Dense(Nc, activation='tanh'))(x)\n",
        "        attention = keras.layers.TimeDistributed(keras.layers.Dense(1, activation='tanh'))(h)\n",
        "        attention = keras.layers.Flatten()(attention)  \n",
        "        attention = keras.layers.Softmax(axis=1, name='attention')(attention) # normalize attention values\n",
        "        attention = keras.layers.RepeatVector(Nc)(attention)\n",
        "        attention = keras.layers.Permute([2, 1])(attention)\n",
        "        representation = keras.layers.multiply([h, attention])\n",
        "        representation = tf.math.reduce_sum(representation, axis = 1)\n",
        "        x = representation\n",
        "    else:\n",
        "        x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    if num_dense:\n",
        "        x = keras.layers.Dense(num_dense, activation = 'relu')(x)\n",
        "    if dropout_rate:\n",
        "        x = keras.layers.Dropout(Params[dropout_rate])(x)\n",
        "\n",
        "    if regress:\n",
        "        # finalOut = keras.layers.Dense(1, activation=linear01)(x)\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if singleclass:\n",
        "        finalOut = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    if multiclass:\n",
        "        finalOut = keras.layers.Dense(nclasses, activation='softmax')(x)\n",
        "\n",
        "    out1 = keras.layers.Dense(1, activation='sigmoid', name='outfirst')(x)\n",
        "    out2 = keras.layers.Dense(1, activation='sigmoid', name='outpeak')(x)\n",
        "    # define the model's start and end points    \n",
        "    model = keras.Model(inpTensor,[out1,out2])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsAfrYxP75be"
      },
      "outputs": [],
      "source": [
        "# These parameters are currently hard-coded\n",
        "ENCDIM = 1500\n",
        "NC = 300\n",
        "NL = 1\n",
        "NT = 1\n",
        "NHEADS = 8\n",
        "FFDIM = 64\n",
        "NDENSE = 64\n",
        "TRANSDROPRATE = 0.1\n",
        "DROPRATE = 0.0\n",
        "\n",
        "LEARN_RATE = 0.0001\n",
        "\n",
        "BATCH_SIZE = 48\n",
        "\n",
        "STEPS_PER_EXECUTION = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset Preprocessing"
      ],
      "metadata": {
        "id": "i-G4mKW85oKz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40aw2aurV8dQ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(FILELOC + \"species_dataset_20211127.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVSUSZWSWVMA"
      },
      "outputs": [],
      "source": [
        "dataset = data[(data.Length>1000) & (data.Species.str.contains('coronavirus'))].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-iZCINgY6_J"
      },
      "outputs": [],
      "source": [
        "dataset['Label'] = dataset.Host.apply(lambda x: 1 if x=='Homo sapiens' else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgaptcB8Eqxa"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    if x=='Alphacoronavirus':\n",
        "        return 0\n",
        "    if x=='Betacoronavirus':\n",
        "        return 1\n",
        "    if x=='Gammacoronavirus':\n",
        "        return 2\n",
        "\n",
        "dataset['genuslabel'] = dataset.Genus.apply(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG1pPfZ2EqtF"
      },
      "outputs": [],
      "source": [
        "# dataset.to_csv(FILELOC + 'species_dataset_2021110.csv')\n",
        "# np.savetxt(FILELOC + 'species_dataset_2021110_genuslabels.csv', dataset.genuslabel.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzYSbQ3Al9Bw"
      },
      "outputs": [],
      "source": [
        "dataset['date'] = dataset['Release_Date'].apply(lambda x:dateparse(x))\n",
        "dataset['date'] = dataset['date'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB5fHqICEAtX",
        "outputId": "a1ec8f67-f79a-432e-ba6c-c6e3a59dee56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1138"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset[dataset.Collection_Date.isna()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxQnsqX2jwFn"
      },
      "outputs": [],
      "source": [
        "# dataset.to_csv(FILELOC + 'species_dataset_2021111.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVPr8dQU8yEN"
      },
      "outputs": [],
      "source": [
        "# np.savetxt(FILELOC + 'species_dataset_2021111_genuslabels.csv', dataset.genuslabel.values)\n",
        "# with open(FILELOC + 'species_dataset_2021111_data.csv', 'w') as f:\n",
        "#     csv.writer(f).writerows(dataset.Seq.values)\n",
        "# np.savetxt(FILELOC + 'species_dataset_2021111_labels.csv', dataset.Label.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVq7j7X8p_5L"
      },
      "outputs": [],
      "source": [
        "SEQLEN = 1500\n",
        "\n",
        "def f(x):\n",
        "    if len(x) < SEQLEN:\n",
        "        return x + '*'*(SEQLEN-len(x))\n",
        "    elif len(x) > SEQLEN:\n",
        "        return x[:SEQLEN]\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "dataset['PadSeq'] = dataset['Seq'].apply(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "S5YYy5Cz5TTQ",
        "outputId": "d61dbfcc-b76c-436e-db7e-ced7556d2c73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb3904422d0>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset.Seq.apply(len).plot.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is how to do a label, where 1 is Human and 0 is Non-Human"
      ],
      "metadata": {
        "id": "OCc2xIbu5tWU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWuYBAvbkOaU"
      },
      "outputs": [],
      "source": [
        "dataset['hostlabel'] = dataset.Host.apply(lambda x: 1 if x=='Homo sapiens' else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is how to do a label for coronavirus genus, where 0, 1, 2, and 3 are different coronavirus genuses"
      ],
      "metadata": {
        "id": "RbuoG-x65weJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPnYR6h1kOac"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "    if x=='Alphacoronavirus':\n",
        "        return 0\n",
        "    if x=='Betacoronavirus':\n",
        "        return 1\n",
        "    if x=='Gammacoronavirus':\n",
        "        return 2\n",
        "    if x=='Deltacoronavirus':\n",
        "        return 3\n",
        "\n",
        "dataset['genuslabel'] = dataset.Genus.apply(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMWjKs41kOae"
      },
      "outputs": [],
      "source": [
        "dataset['date'] = dataset['Release_Date'].apply(lambda x:dateparse(x))\n",
        "dataset['date'] = dataset['date'].dt.date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiRleV1h3QHw"
      },
      "source": [
        "#Taxonomic Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqXh0Ei76tXD"
      },
      "outputs": [],
      "source": [
        "ismlen = 1500\n",
        "\n",
        "def tokenize_sequences(data_dataframe, SeqCol='ISM', seqlen=1273):\n",
        "    def f(x):\n",
        "        if len(x) < seqlen:\n",
        "            return x + '*'*(seqlen-len(x))\n",
        "        elif len(x) > seqlen:\n",
        "            return x[:seqlen]\n",
        "        else:\n",
        "            return x\n",
        "    data = np.vstack(data_dataframe[SeqCol].apply(f).apply(lambda x: np.array(list(x))))\n",
        "    aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "            'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "            'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "            ]\n",
        "    aa_tokenizer = {aa_list[k]:k+1 for k in range(len(aa_list))}\n",
        "    aa_tokenizer['*'] = 0\n",
        "    aa_tokenizer['X'] = 0\n",
        "    # optionally handle B, J, Z ambiguities\n",
        "    # Asx\tB\tAspartic acid or Asparagine (D or N)\n",
        "    # Glx\tZ\tGlutamic acid or Glutamine (E or Q)\n",
        "    # Xaa\tX\tAny amino acid\n",
        "    # Xle\tJ\tLeucine or Isoleucine (L or I)\n",
        "    aa_tokenizer['B'] = 0\n",
        "    aa_tokenizer['Z'] = 0\n",
        "    aa_tokenizer['J'] = 0\n",
        "\n",
        "    return np.vectorize(aa_tokenizer.get)(data)\n",
        "\n",
        "aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "        'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "        'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPSk4dLObrZM"
      },
      "outputs": [],
      "source": [
        "datadf['seqlen'] = datadf.Seq.apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfM-NmFNf8He"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(dataset, 'Seq', 1500)\n",
        "y = dataset.genuslabel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIjNQzFbcBN2"
      },
      "outputs": [],
      "source": [
        "seqtok = tokenize_sequences(datadf[datadf.seqlen>1000], 'Seq', 1500)\n",
        "y = datadf[datadf.seqlen>1000].genuslabel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually create train-test split indices"
      ],
      "metadata": {
        "id": "T4Yn1LUJ7b7e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnHgQJXxgOPg"
      },
      "outputs": [],
      "source": [
        "trainindex = np.random.choice(range(len(seqtok)), size = int(0.9*len(seqtok)), replace=False)\n",
        "testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "\n",
        "# save train index for future use:\n",
        "np.savetxt(FILELOC + 'species_trainindex_2021121.csv', trainindex, fmt='%i', delimiter=',')\n",
        "trainindex = np.loadtxt(FILELOC + 'species_reduced_trainindex_2021121.csv', dtype=int, delimiter=',')\n",
        "\n",
        "testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "xtraintok = seqtok[trainindex]; ytrain = y[trainindex]\n",
        "xtesttok = seqtok[testindex]; ytest = y[testindex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2D1JqUZgsxf",
        "outputId": "3fa3f490-6820-4c60-a24e-f2ebf81386ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.1203703703703705, 0.6364894795127354, 0.8668929110105581, 2.6125]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = list(class_weight.compute_class_weight(class_weight='balanced',\n",
        "                                                       classes=np.unique(y), y=y))\n",
        "sample_weights = np.array([class_weights[int(yi)] for yi in y])\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qNvsD6U7Ej5"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 48\n",
        "VAL_SPLIT = 0.2\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',\n",
        "    verbose = 1,\n",
        "    patience = 10, #5,\n",
        "    mode = 'auto',\n",
        "    min_delta = 0,\n",
        "    restore_best_weights = True\n",
        "    )\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "if tpu_env:\n",
        "    with tpu_strategy.scope():\n",
        "        model = reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                            output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "else:\n",
        "    model = reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                        output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "history = model.fit(xtraintok, ytrain,\n",
        "                    sample_weight = sample_weights,\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    epochs = NUM_EPOCHS,\n",
        "                    verbose = 1,\n",
        "                    # validation_split = VAL_SPLIT,\n",
        "                    # callbacks = [early_stopping],\n",
        "                    )\n",
        "\n",
        "# to save weights of the model\n",
        "model.save_weights(f\"{FILELOC}taxonomy_weights.h5\", save_format='h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mRTtE8PP7OeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FVaog_BN7Oan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K1XtrNTLDDZ"
      },
      "source": [
        "##Load Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySoJYkQcLJB7"
      },
      "outputs": [],
      "source": [
        "ismlen = 1500\n",
        "\n",
        "def tokenize_sequences(data_dataframe, SeqCol='ISM', seqlen=1273):\n",
        "    def f(x):\n",
        "        if len(x) < seqlen:\n",
        "            return x + '*'*(seqlen-len(x))\n",
        "        elif len(x) > seqlen:\n",
        "            return x[:seqlen]\n",
        "        else:\n",
        "            return x\n",
        "    data = np.vstack(data_dataframe[SeqCol].apply(f).apply(lambda x: np.array(list(x))))\n",
        "    aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "            'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "            'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "            ]\n",
        "    aa_tokenizer = {aa_list[k]:k+1 for k in range(len(aa_list))}\n",
        "    aa_tokenizer['*'] = 0\n",
        "    aa_tokenizer['X'] = 0\n",
        "    # optionally handle B, J, Z ambiguities\n",
        "    # Asx\tB\tAspartic acid or Asparagine (D or N)\n",
        "    # Glx\tZ\tGlutamic acid or Glutamine (E or Q)\n",
        "    # Xaa\tX\tAny amino acid\n",
        "    # Xle\tJ\tLeucine or Isoleucine (L or I)\n",
        "    aa_tokenizer['B'] = 0\n",
        "    aa_tokenizer['Z'] = 0\n",
        "    aa_tokenizer['J'] = 0\n",
        "\n",
        "    return np.vectorize(aa_tokenizer.get)(data)\n",
        "\n",
        "aa_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E',\n",
        "        'G', 'H', 'I', 'L', 'K', 'M', 'F',\n",
        "        'P', 'S', 'T', 'W', 'Y', 'V', '-',\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2oB88Hchbzb"
      },
      "source": [
        "Analysis of error only for multiple runs of Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaZZvMIXh1a9"
      },
      "outputs": [],
      "source": [
        "# # align = pd.read_csv(FILELOC+'align1001_all.csv')\n",
        "# raw = pd.read_csv(FILELOC + 'results1001_Spike.csv')\n",
        "# raw.drop_duplicates('Seq', inplace=True)\n",
        "\n",
        "# raw['seqlen'] = raw.Spike.apply(len)\n",
        "# raw['X'] = raw.Spike.apply(lambda x: x.count('X'))\n",
        "# print(len(raw), len(raw[raw.X > 0]), len(raw[raw.seqlen.between(1271,1273)]), len(raw[raw.seqlen<1000]))\n",
        "\n",
        "# raw_sample = raw.sample(100000)\n",
        "# raw_short = raw[raw.seqlen<1000]\n",
        "# raw_short.drop_duplicates('Spike', inplace=True)\n",
        "# raw_noise.drop_duplicates('Spike', inplace=True)\n",
        "\n",
        "# with open(f'{FILELOC}coronovirus_spike_taxonomy_sequence_samples.pkl', 'wb') as f:\n",
        "#     pickle.dump([raw_sample, align_sample, raw_short, align_short, align_noise], f)\n",
        "\n",
        "with open(f'{FILELOC}coronovirus_spike_taxonomy_sequence_samples.pkl', 'rb') as f:\n",
        "    rawsample, rawshort, rawnoise = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga-TKRBS4YxL"
      },
      "outputs": [],
      "source": [
        "align = pd.read_csv(FILELOC+'align1001_all.csv')\n",
        "df = align['MaskedSeq'].value_counts().to_frame().rename(columns={\"MaskedSeq\":\"Count\"})\n",
        "# mdf = pd.read_csv(FILELOC + 'metadata.tsv', '\\t')\n",
        "# mdf = pd.read_csv(FILELOC + 'covid_alldata_merged_align1001.csv')\n",
        "with open(FILELOC + 'aligned_sequence_lineages_1001.pkl', 'rb') as f:\n",
        "    mdf,_ = pickle.load(f)\n",
        "df = df.join(mdf).reset_index(drop=False).rename(columns={'index':'MaskedSeq'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqh8n9SxFils"
      },
      "outputs": [],
      "source": [
        "lin = ['B.1.526', 'B.1.427', 'B.1.617.2', 'AY.4', 'B.1.351', 'B.1', 'A', 'P.1', 'B.1.1.7', 'C.37']\n",
        "indices = []\n",
        "for l in lin:\n",
        "    indices.append(df[(df.Lineage==l) & (df.Count==df[df.Lineage==l]['Count'].max())].index[0])\n",
        "tdf = df.loc[indices].copy()\n",
        "tdf.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ2WNspGkl3j"
      },
      "outputs": [],
      "source": [
        "REFSPIKE = 'MFVFLVLLPLVSSQCVNLTTRTQLPPAYTNSFTRGVYYPDKVFRSSVLHSTQDLFLPFFSNVTWFHAIHVSGTNGTKRFDNPVLPFNDGVYFASTEKSNIIRGWIFGTTLDSKTQSLLIVNNATNVVIKVCEFQFCNDPFLGVYYHKNNKSWMESEFRVYSSANNCTFEYVSQPFLMDLEGKQGNFKNLREFVFKNIDGYFKIYSKHTPINLVRDLPQGFSALEPLVDLPIGINITRFQTLLALHRSYLTPGDSSSGWTAGAAAYYVGYLQPRTFLLKYNENGTITDAVDCALDPLSETKCTLKSFTVEKGIYQTSNFRVQPTESIVRFPNITNLCPFGEVFNATRFASVYAWNRKRISNCVADYSVLYNSASFSTFKCYGVSPTKLNDLCFTNVYADSFVIRGDEVRQIAPGQTGKIADYNYKLPDDFTGCVIAWNSNNLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGSTPCNGVEGFNCYFPLQSYGFQPTNGVGYQPYRVVVLSFELLHAPATVCGPKKSTNLVKNKCVNFNFNGLTGTGVLTESNKKFLPFQQFGRDIADTTDAVRDPQTLEILDITPCSFGGVSVITPGTNTSNQVAVLYQDVNCTEVPVAIHADQLTPTWRVYSTGSNVFQTRAGCLIGAEHVNNSYECDIPIGAGICASYQTQTNSPRRARSVASQSIIAYTMSLGAENSVAYSNNSIAIPTNFTISVTTEILPVSMTKTSVDCTMYICGDSTECSNLLLQYGSFCTQLNRALTGIAVEQDKNTQEVFAQVKQIYKTPPIKDFGGFNFSQILPDPSKPSKRSFIEDLLFNKVTLADAGFIKQYGDCLGDIAARDLICAQKFNGLTVLPPLLTDEMIAQYTSALLAGTITSGWTFGAGAALQIPFAMQMAYRFNGIGVTQNVLYENQKLIANQFNSAIGKIQDSLSSTASALGKLQDVVNQNAQALNTLVKQLSSNFGAISSVLNDILSRLDKVEAEVQIDRLITGRLQSLQTYVTQQLIRAAEIRASANLAATKMSECVLGQSKRVDFCGKGYHLMSFPQSAPHGVVFLHVTYVPAQEKNFTTAPAICHDGKAHFPREGVFVSNGTHWFVTQRNFYEPQIITTDNTFVSGNCDVVIGIVNNTVYDPLQPELDSFKEELDKYFKNHTSPDVDLGDISGINASVVNIQKEIDRLNEVAKNLNESLIDLQELGKYEQYIKWPWYIWLGFIAGLIAIVMVTIMLCCMTSCCSCLKGCCSCGSCCKFDEDDSEPVLKGVKLHYT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgvET_SAkoYK"
      },
      "outputs": [],
      "source": [
        "tdf.loc[len(tdf.index)] = [REFSPIKE, 1, 'Wu-Ref']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9wYuh9JlAHU"
      },
      "outputs": [],
      "source": [
        "omicron = pd.read_csv(FILELOC + 'omicron_spike_20211209.csv')\n",
        "omiseq = omicron['Align'].value_counts().index[0]\n",
        "tdf.loc[len(tdf.index)] = [omiseq, omicron['Align'].value_counts().values[0], 'Omicron']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp-dXVGIjKku"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBXe44AzL7oJ"
      },
      "outputs": [],
      "source": [
        "# run = 8\n",
        "run = 1\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "with tpu_strategy.scope():\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    # model.load_weights(f\"{FILELOC}taxonomy_weights_20211213_{run}.h5\")\n",
        "    # model.load_weights(f\"{FILELOC}taxonomy_weights_20211221_{run}.h5\")\n",
        "    model.load_weights(f\"{FILELOC}taxonomy_weights_20220112_seq1000_{run}.h5\")\n",
        "    model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kARw7DUzNfWE",
        "outputId": "05a48a81-5f72-4788-9d09-efebca41ce4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f08282c34d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f08282c34d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0828117170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0828117170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "tok = tokenize_sequences(tdf,'MaskedSeq',1500)\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()\n",
        "    pred = model.predict(tok)\n",
        "    emb = get_embedding_model.predict(tok)\n",
        "    att = get_attention_model.predict(tok)\n",
        "    tdf['pred'] = [p for p in pred]\n",
        "    tdf['emb'] = [e for e in emb]\n",
        "    tdf['att'] = [a for a in att]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXjnNBtE8f8S"
      },
      "outputs": [],
      "source": [
        "sdf = pd.read_csv(FILELOC + \"species_dataset_20211127.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0eVRRo8hKb"
      },
      "outputs": [],
      "source": [
        "run = 1\n",
        "tf.keras.backend.clear_session()\n",
        "with tpu_strategy.scope():\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    model.load_weights(f\"{FILELOC}taxonomy_weights_20220112_seq1000_{run}.h5\")\n",
        "    model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRJWe8_a8jBe",
        "outputId": "0b641cf2-a1fb-481f-c006-0a1844652e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2975: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2975: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        }
      ],
      "source": [
        "tok = tokenize_sequences(sdf,'Seq',1500)\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()\n",
        "    pred = model.predict(tok)\n",
        "    emb = get_embedding_model.predict(tok)\n",
        "    att = get_attention_model.predict(tok)\n",
        "    sdf['pred'] = [p for p in pred]\n",
        "    sdf['emb'] = [e for e in emb]\n",
        "    sdf['att'] = [a for a in att]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5Qt35ibnYfF"
      },
      "outputs": [],
      "source": [
        "# with open(FILELOC + 'topsequences_taxonomy_20220112.pkl', 'wb') as f:\n",
        "#     pickle.dump(tdf, f)\n",
        "# with open(FILELOC + 'topsequences_taxonomy_20220112.pkl', 'rb') as f:\n",
        "#     tdf = pickle.load(f)\n",
        "\n",
        "# with open(FILELOC + 'topsequences_taxonomy_20220112_seq1000.pkl', 'wb') as f:\n",
        "#     pickle.dump(tdf, f)\n",
        "with open(FILELOC + 'topsequences_taxonomy_20220112_seq1000.pkl', 'rb') as f:\n",
        "    tdf = pickle.load(f)\n",
        "\n",
        "# with open(FILELOC + 'allsequences_taxonomy_20220112_seq1000.pkl', 'wb') as f:\n",
        "    # pickle.dump(sdf, f)\n",
        "with open(FILELOC + 'allsequences_taxonomy_20220112_seq1000.pkl', 'rb') as f:\n",
        "    sdf = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hz2ZWeh8Z7k"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwYOzHRK75wv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g169PWV75tH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs-KcpqA75pu"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlMkXKhy75mb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im-VigyF75jv"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SAu3OhhyC87",
        "outputId": "d46843e6-4219-4f26-d5aa-2e1dc8ab5380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 16s 77ms/step\n",
            "10/10 [==============================] - 5s 445ms/step\n",
            "1042/1042 [==============================] - 51s 49ms/step\n",
            "149/149 [==============================] - 10s 62ms/step\n",
            "382/382 [==============================] - 18s 46ms/step\n",
            "Dataset   |    Error    |    Dataset size\n",
            "Run 1\n",
            "Patient Validation Data  |  0.9401432380155784  |  19129\n",
            "Patient Training Data  |  0.9667774086378738  |  903\n",
            "Raw Sample  |  0.97937  |  100000\n",
            "Raw Short Sequences  |  0.433431744692816  |  14226\n",
            "Raw Noisy Sequences  |  0.8557429850420352  |  36636\n",
            "Run 2\n",
            "Patient Validation Data  |  0.9447958596894767  |  19129\n",
            "Patient Training Data  |  0.9656699889258029  |  903\n",
            "Raw Sample  |  0.98395  |  100000\n",
            "Raw Short Sequences  |  0.4519893153381133  |  14226\n",
            "Raw Noisy Sequences  |  0.9232721912872585  |  36636\n",
            "Run 3\n",
            "Patient Validation Data  |  0.9461027758900099  |  19129\n",
            "Patient Training Data  |  0.9656699889258029  |  903\n",
            "Raw Sample  |  0.9857  |  100000\n",
            "Raw Short Sequences  |  0.4673836637143259  |  14226\n",
            "Raw Noisy Sequences  |  0.9331258871055792  |  36636\n",
            "Run 4\n",
            "Patient Validation Data  |  0.9485597783470124  |  19129\n",
            "Patient Training Data  |  0.9667774086378738  |  903\n",
            "Raw Sample  |  0.98809  |  100000\n",
            "Raw Short Sequences  |  0.4826374244341347  |  14226\n",
            "Raw Noisy Sequences  |  0.9639971612621465  |  36636\n",
            "Run 5\n",
            "Patient Validation Data  |  0.9460504992419886  |  19129\n",
            "Patient Training Data  |  0.9667774086378738  |  903\n",
            "Raw Sample  |  0.9865  |  100000\n",
            "Raw Short Sequences  |  0.5074511457893996  |  14226\n",
            "Raw Noisy Sequences  |  0.9233813735123921  |  36636\n",
            "Run 6\n",
            "Patient Validation Data  |  0.9437503267290501  |  19129\n",
            "Patient Training Data  |  0.9656699889258029  |  903\n",
            "Raw Sample  |  0.98391  |  100000\n",
            "Raw Short Sequences  |  0.49205679741318714  |  14226\n",
            "Raw Noisy Sequences  |  0.8931924882629108  |  36636\n",
            "Run 7\n",
            "Patient Validation Data  |  0.9429139003607089  |  19129\n",
            "Patient Training Data  |  0.9667774086378738  |  903\n",
            "Raw Sample  |  0.98253  |  100000\n",
            "Raw Short Sequences  |  0.5024602839870659  |  14226\n",
            "Raw Noisy Sequences  |  0.856862102849656  |  36636\n",
            "Run 8\n",
            "Patient Validation Data  |  0.9476710753306498  |  19129\n",
            "Patient Training Data  |  0.9667774086378738  |  903\n",
            "Raw Sample  |  0.987  |  100000\n",
            "Raw Short Sequences  |  0.5270631238577252  |  14226\n",
            "Raw Noisy Sequences  |  0.9219620045856535  |  36636\n",
            "Run 9\n",
            "Patient Validation Data  |  0.9425479638245595  |  19129\n",
            "Patient Training Data  |  0.9656699889258029  |  903\n",
            "Raw Sample  |  0.98235  |  100000\n",
            "Raw Short Sequences  |  0.4566990018276395  |  14226\n",
            "Raw Noisy Sequences  |  0.8897805437274812  |  36636\n",
            "Run 10\n",
            "Patient Validation Data  |  0.9406137278477704  |  19129\n",
            "Patient Training Data  |  0.9656699889258029  |  903\n",
            "Raw Sample  |  0.97972  |  100000\n",
            "Raw Short Sequences  |  0.4518487276817095  |  14226\n",
            "Raw Noisy Sequences  |  0.8404574735233103  |  36636\n"
          ]
        }
      ],
      "source": [
        " from collections import Counter\n",
        "\n",
        "# error = {}\n",
        "# for run in range(1,11):\n",
        "for run in [9]:\n",
        "    error[run] = {}\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    with tpu_strategy.scope():\n",
        "        model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                        output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "        # model.load_weights(f\"{FILELOC}taxonomy_weights_20211213_{run}.h5\")\n",
        "        model.load_weights(f\"{FILELOC}taxonomy_weights_20211221_{run}.h5\")        \n",
        "        model.compile()\n",
        "\n",
        "        vdf = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_valid1001_raw_sequences.csv')\n",
        "        vdf.drop(columns='sample_weight',inplace=True)\n",
        "        valtok = tokenize_sequences(vdf, SeqCol='ISM', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Patient Validation Data\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)]\n",
        "\n",
        "        vdf = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_raw_sequences.csv')\n",
        "        vdf.drop(columns='sample_weight',inplace=True)\n",
        "        valtok = tokenize_sequences(vdf, SeqCol='ISM', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Patient Training Data\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)]\n",
        "\n",
        "        vdf = rawsample\n",
        "        valtok = tokenize_sequences(vdf, 'Spike', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Raw Sample\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)]\n",
        "\n",
        "        vdf = rawshort\n",
        "        valtok = tokenize_sequences(vdf, 'Spike', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Raw Short Sequences\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)]\n",
        "\n",
        "        vdf = rawnoise\n",
        "        valtok = tokenize_sequences(vdf, 'Spike', seqlen=1500)\n",
        "        pred = model.predict(valtok, verbose=1, batch_size=96)\n",
        "        vdf['Predict_Genus'] = pred.argmax(axis=1)\n",
        "        error[run][\"Raw Noisy Sequences\"] = [Counter(vdf.Predict_Genus)[1] / len(vdf), len(vdf), Counter(vdf.Predict_Genus)] \n",
        "\n",
        "datatypes = list(error[1].keys())\n",
        "mean_error = {d:np.mean([error[k][d][0] for k in range(1,11)]) for d in datatypes}\n",
        "std_error = {d:np.std([error[k][d][0] for k in range(1,11)]) for d in datatypes}\n",
        "\n",
        "print(\"Dataset   |    Error    |    Dataset size\")\n",
        "for run in range(1,11):\n",
        "    print(f\"Run {run}\")\n",
        "    for e,v in error[run].items():\n",
        "        print(f\"{e}  |  {v[0]}  |  {v[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v182xLislnnH"
      },
      "outputs": [],
      "source": [
        "with open(FILELOC + \"covid_taxonomy_error_trans_20211221.pkl\", \"wb\") as f:\n",
        "    pickle.dump([error, datatypes, mean_error, std_error], f)\n",
        "# with open(FILELOC + \"covid_taxonomy_error_trans.pkl\", \"wb\") as f:\n",
        "#     pickle.dump([error, datatypes, mean_error, std_error], f)\n",
        "# with open(FILELOC + \"covid_taxonomy_error_trans.pkl\", \"rb\") as f:\n",
        "#     error, datatypes, mean_error, std_error = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySSPcwCxQWZ0",
        "outputId": "34f0c4a0-dee5-4374-8a43-8bd289da39fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 [0.433431744692816, 14226, Counter({2: 8056, 1: 6166, 0: 4})]\n",
            "2 [0.4519893153381133, 14226, Counter({2: 7699, 1: 6430, 0: 97})]\n",
            "3 [0.4673836637143259, 14226, Counter({2: 7376, 1: 6649, 0: 201})]\n",
            "4 [0.4826374244341347, 14226, Counter({2: 7261, 1: 6866, 0: 99})]\n",
            "5 [0.5074511457893996, 14226, Counter({1: 7219, 2: 6795, 0: 212})]\n",
            "6 [0.49205679741318714, 14226, Counter({1: 7000, 2: 6971, 0: 255})]\n",
            "7 [0.5024602839870659, 14226, Counter({1: 7148, 2: 6940, 0: 138})]\n",
            "8 [0.5270631238577252, 14226, Counter({1: 7498, 2: 6590, 0: 138})]\n",
            "9 [0.4566990018276395, 14226, Counter({2: 7493, 1: 6497, 0: 236})]\n",
            "10 [0.4518487276817095, 14226, Counter({2: 7288, 1: 6428, 0: 510})]\n"
          ]
        }
      ],
      "source": [
        "for r in error:\n",
        "    print(r, error[r]['Raw Short Sequences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGWeui39QWWn",
        "outputId": "848f7f7d-6f4b-4bde-ad3d-7a9fafb47e5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Patient Training Data': 0.0005537098560354226,\n",
              " 'Patient Validation Data': 0.002687528187889592,\n",
              " 'Raw Noisy Sequences': 0.03778744888055464,\n",
              " 'Raw Sample': 0.0028185095351976473,\n",
              " 'Raw Short Sequences': 0.02828481535845419}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "std_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnQuSLzSJ9NK"
      },
      "outputs": [],
      "source": [
        "with open(f'{FILELOC}coronovirus_spike_taxonomy_sequence_samples.pkl', 'rb') as f:\n",
        "    rawsample, rawshort, rawnoise = pickle.load(f)\n",
        "pat_val = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_valid1001_raw_sequences.csv')\n",
        "pat_train = pd.read_csv(FILELOC + 'covid_rawseqs_crossval/covid_patient_0912_raw_sequences.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRPQow3NOoS4"
      },
      "outputs": [],
      "source": [
        "run = 8\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "with tpu_strategy.scope():\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    # model.load_weights(f\"{FILELOC}taxonomy_weights_20211213_{run}.h5\")\n",
        "    model.load_weights(f\"{FILELOC}taxonomy_weights_20211221_{run}.h5\")\n",
        "    model.compile()\n",
        "\n",
        "datatype = ['Patient Training Data', 'Patient Validation Data',\n",
        "            'Raw Noisy Sequences', 'Raw Sample', 'Raw Short Sequences']\n",
        "datasrc = [pat_train, pat_val, rawnoise, rawsample, rawshort]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76XNvCDXJyZJ",
        "outputId": "d92971b6-3f5c-47d0-fca3-b59d08cb1bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 10s 3s/step\n",
            "4/4 [==============================] - 9s 3s/step\n",
            "4/4 [==============================] - 8s 2s/step\n",
            "75/75 [==============================] - 15s 192ms/step\n",
            "75/75 [==============================] - 15s 192ms/step\n",
            "75/75 [==============================] - 14s 187ms/step\n",
            "144/144 [==============================] - 23s 158ms/step\n",
            "144/144 [==============================] - 23s 159ms/step\n",
            "144/144 [==============================] - 23s 156ms/step\n",
            "391/391 [==============================] - 60s 152ms/step\n",
            "391/391 [==============================] - 60s 152ms/step\n",
            "391/391 [==============================] - 59s 149ms/step\n",
            "56/56 [==============================] - 12s 207ms/step\n",
            "56/56 [==============================] - 12s 205ms/step\n",
            "56/56 [==============================] - 11s 199ms/step\n"
          ]
        }
      ],
      "source": [
        "pred = {}; emb = {}; att = {}\n",
        "with tpu_strategy.scope():\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()\n",
        "    for d in range(len(datatype)):\n",
        "        if 'Patient' in datatype[d]:\n",
        "            tok = tokenize_sequences(datasrc[d], 'ISM', seqlen=1500)\n",
        "        else:\n",
        "            tok = tokenize_sequences(datasrc[d], 'Spike', seqlen=1500)\n",
        "        pred[datatype[d]] = model.predict(tok, verbose=True, batch_size=32*8)\n",
        "        emb[datatype[d]] = get_embedding_model.predict(tok, verbose=True, batch_size=32*8)\n",
        "        att[datatype[d]] = get_attention_model.predict(tok, verbose=True, batch_size=32*8)\n",
        "\n",
        "with open(FILELOC + 'covid_taxonomy_model_results_20211221.pkl', 'wb') as f:\n",
        "    pickle.dump([datatype, pred, emb, att], f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-EckhxJZMIN"
      },
      "source": [
        "###Show results at the genus level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q1cctUugjcL"
      },
      "outputs": [],
      "source": [
        "datadf = pd.read_csv(FILELOC + 'species_dataset_reduced_20211127_dropduplicates.csv')\n",
        "seqtok = tokenize_sequences(datadf, 'Seq', 1500)\n",
        "y = datadf.genuslabel\n",
        "trainindex = np.loadtxt(FILELOC + 'species_reduced_trainindex_20211127_dropduplicates.csv', dtype=int, delimiter=',')\n",
        "testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "xtraintok = seqtok[trainindex]; ytrain = y[trainindex]\n",
        "xtesttok = seqtok[testindex]; ytest = y[testindex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6SpCKT2G9hK",
        "outputId": "6085e26b-70f9-4550-b28f-b4258774a7ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 11s 498ms/step\n",
            "22/22 [==============================] - 11s 475ms/step\n"
          ]
        }
      ],
      "source": [
        "run = 8\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "with tpu_strategy.scope():\n",
        "    model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "                    output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "    model.load_weights(f\"{FILELOC}taxonomy_weights_20211221_{run}.h5\")\n",
        "    model.compile()\n",
        "\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()\n",
        "    emb = get_embedding_model.predict(seqtok, verbose=True, batch_size = 32*8)\n",
        "    att = get_attention_model.predict(seqtok, verbose=True, batch_size = 32*8)\n",
        "\n",
        "with open(FILELOC + 'covid_taxonomy_model_scratch_genus_20211221.pkl', 'wb') as f:\n",
        "    pickle.dump([datadf, emb, att],f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IzDu6vnGoJA",
        "outputId": "0e6e85a9-1a88-445d-e84f-e54e8e4779f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2975: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2975: StrategyBase.unwrap (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "use `experimental_local_results` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20/20 [==============================] - 10s 493ms/step\n",
            "3/3 [==============================] - 5s 2s/step\n",
            "20/20 [==============================] - 10s 479ms/step\n",
            "3/3 [==============================] - 5s 2s/step\n"
          ]
        }
      ],
      "source": [
        "# run = 3\n",
        "\n",
        "# tf.keras.backend.clear_session()\n",
        "# with tpu_strategy.scope():\n",
        "#     model= reset_model(regress=False, singleclass=False, multiclass=True,\n",
        "#                     output_multiheadatt=False, use_att=True, nclasses=4)\n",
        "#     model.load_weights(f\"{FILELOC}taxonomy_weights_20211213_{run}.h5\")\n",
        "#     model.compile()\n",
        "\n",
        "#     get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "#     get_embedding_model.compile()\n",
        "#     get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "#     get_attention_model.compile()\n",
        "#     embtrain = get_embedding_model.predict(xtraintok, verbose=True, batch_size = 32*8)\n",
        "#     embtest = get_embedding_model.predict(xtesttok, verbose=True, batch_size = 32*8)\n",
        "#     trainatt = get_attention_model.predict(xtraintok, verbose=True, batch_size = 32*8)\n",
        "#     testatt = get_attention_model.predict(xtesttok, verbose=True, batch_size = 32*8)\n",
        "\n",
        "# with open(FILELOC + 'covid_taxonomy_model_scratch_genus.pkl', 'wb') as f:\n",
        "#     pickle.dump([datadf, trainindex, testindex, embtrain, embtest, trainatt, testatt],f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzScX96mgZ0B"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y16IFzshgZug"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYZi0qG2gZrw"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eHGspSZgZo4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCD4j1OYFy6j"
      },
      "outputs": [],
      "source": [
        "with tpu_strategy.scope():\n",
        "    get_embedding_model = keras.Model(inputs=model.input,outputs=model.get_layer('dense_4').output)\n",
        "    get_embedding_model.compile()\n",
        "    get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "    get_attention_model.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWHuUoASVaRH"
      },
      "outputs": [],
      "source": [
        "datadf = pd.read_csv(FILELOC + 'species_dataset_reduced_20211127.csv')\n",
        "seqtok = tokenize_sequences(datadf, 'Seq', 1500)\n",
        "y = datadf.genuslabel\n",
        "trainindex = np.loadtxt(FILELOC + 'species_reduced_trainindex_20211127.csv', dtype=int, delimiter=',')\n",
        "testindex = [k for k in range(len(seqtok)) if k not in trainindex]\n",
        "xtraintok = seqtok[trainindex]; ytrain = y[trainindex]\n",
        "xtesttok = seqtok[testindex]; ytest = y[testindex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OflnBGuzVTMc"
      },
      "outputs": [],
      "source": [
        "omicron = pd.read_csv(FILELOC + \"omicron_spike_20211128.csv\")\n",
        "otok = tokenize_sequences(omicron,'Spike',1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WykEmet6aac5"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "pred = model.predict(otok,verbose=1,batch_size=32)\n",
        "Counter(pred.argmax(axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd8B6mMnVMho",
        "outputId": "2aad7bba-72f6-473a-aeb4-0d96af1b6a49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 6s 215ms/step\n"
          ]
        }
      ],
      "source": [
        "emb = get_embedding_model.predict(np.vstack([xtesttok,otok]), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR6Anne1en5K",
        "outputId": "61a872eb-e6e4-4656-c88d-9b62ff97de48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:783: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/manifold/_t_sne.py:793: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "t = TSNE(n_components=2).fit_transform(emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voC2dMwOe5e5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkT50gYsen1U"
      },
      "outputs": [],
      "source": [
        "# fig, ax = plt.subplots()\n",
        "# fig.set_size_inches(12,8)\n",
        "\n",
        "# genus = ['Alphacoronavirus', 'Betacoronavirus', 'Gammacoronavirus', 'Deltacoronavirus']\n",
        "# for genus_ind in np.unique(ytest):\n",
        "#     selind = np.where(ytest==genus_ind)[0]\n",
        "#     ax.scatter(t[selind,0], t[selind,1], marker='x', label=genus[genus_ind])\n",
        "# ax.scatter(t[len(ytest):,0], t[len(ytest):,1], marker='x', label='Omicron')\n",
        "\n",
        "# ax.legend(bbox_to_anchor=(1.0, 0.9),  framealpha=1.0)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgpgH8cPgQCN",
        "outputId": "cfce864b-3b23-47f2-8b60-fe34daa50cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/25 [==============================] - 6s 210ms/step\n"
          ]
        }
      ],
      "source": [
        "att = get_attention_model.predict(np.vstack([xtesttok,otok]), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_in_TjygbpX",
        "outputId": "c47f30a1-e214-44b2-bdfd-61e99dfed408"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([   1,    5,    9,   15,   17,   19,   21,   23,   28,   29,   31,\n",
              "         32,   41,   42,   46,   47,   54,   57,   58,   60,   62,   63,\n",
              "         66,   73,   81,   83,   84,   93,  101,  117,  118,  121,  123,\n",
              "        127,  128,  132,  133,  147,  160,  171,  183,  201,  202,  223,\n",
              "        228,  232,  238,  240,  241,  245,  254,  280,  282,  285,  297,\n",
              "        319,  329,  381,  384,  397,  403,  432,  445,  451,  460,  473,\n",
              "        484,  496,  506,  507,  519,  521,  534,  537,  552,  572,  577,\n",
              "        584,  604,  607,  628,  634,  640,  645,  652,  658,  667,  671,\n",
              "        677,  692,  725,  728,  730,  732,  734,  739,  745,  749,  756,\n",
              "        778,  793,  823,  836,  847,  865,  882,  890,  894,  918,  958,\n",
              "        959,  961,  992,  994, 1028, 1057, 1061, 1073, 1078, 1113, 1124,\n",
              "       1199, 1208, 1213, 1225, 1227, 1230, 1231, 1232, 1233, 1234, 1236,\n",
              "       1237, 1239, 1240, 1243, 1244, 1246, 1249, 1250, 1260, 1266])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.where((np.mean(att[len(ytest):], axis=0)) > 1/1500)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NezP9-lrT90U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FA0rGx6pT9na"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFbt5QGsT9kl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNA3YSoAF3yJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1muOAhnP2M4"
      },
      "outputs": [],
      "source": [
        "trainindex = np.loadtxt(FILELOC + 'covid_rawseqs_crossval/TrainIndex_0.csv', dtype=int)\n",
        "testindex = np.loadtxt(FILELOC + 'covid_rawseqs_crossval/TestIndex_0.csv', dtype=int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64hueS9N3V4N"
      },
      "outputs": [],
      "source": [
        "XGBRAND = 32754\n",
        "xtrain = seqtok[trainindex]; ytrain = np.round(y[trainindex])\n",
        "xgbmodel = xgboost_setup(random_state = XGBRAND)\n",
        "xgbmodel.fit(xtrain,ytrain, eval_set=[(xtrain,ytrain)],# sample_weight=sample_weights,\n",
        "             eval_metric=['auc'], early_stopping_rounds=10, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxUlHy0G3VyI",
        "outputId": "d5969051-1b51-47c4-9a36-44735ac27577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.69      0.59      0.63        41\n",
            "         1.0       0.70      0.78      0.74        50\n",
            "\n",
            "    accuracy                           0.69        91\n",
            "   macro avg       0.69      0.68      0.68        91\n",
            "weighted avg       0.69      0.69      0.69        91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "xtest = seqtok[testindex]; ytest = np.round(y[testindex])\n",
        "print(classification_report(ytest, xgbmodel.predict(xtest)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz6TRwWC3Vu1"
      },
      "outputs": [],
      "source": [
        ">>> from sklearn.neighbors import KNeighborsClassifier\n",
        ">>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
        ">>> neigh.fit(X, y)\n",
        "KNeighborsClassifier(...)\n",
        ">>> print(neigh.predict([[1.1]]))\n",
        "[0]\n",
        ">>> print(neigh.predict_proba([[0.9]]))\n",
        "[[0.666... 0.333...]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89g6E3Ta3VsA"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh1dtARj3VPi"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(xtrain,ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1sPKL7lTxGa",
        "outputId": "6c10f8a6-acad-4713-a0ab-1a7fec384072"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.62      0.61      0.62        41\n",
            "         1.0       0.69      0.70      0.69        50\n",
            "\n",
            "    accuracy                           0.66        91\n",
            "   macro avg       0.66      0.65      0.66        91\n",
            "weighted avg       0.66      0.66      0.66        91\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(ytest,knn.predict(xtest)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc6EIO_PTxCw",
        "outputId": "48442ff6-7358-47a8-ba0f-65f2e26340d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29/29 [==============================] - 37s 1s/step\n"
          ]
        }
      ],
      "source": [
        "get_attention_model = keras.Model(inputs=model.input,outputs=model.get_layer('attention').output)\n",
        "att = get_attention_model.predict(seqtok, verbose=1, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqDt7DDCVYMZ",
        "outputId": "19fcb481-8a02-4909-aa92-aa3fd338307e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.002287253038957715\n"
          ]
        }
      ],
      "source": [
        "halfatt = 0.5*np.max(att); print(halfatt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqqlL8I7VYJQ",
        "outputId": "a21727ac-7df0-4f30-f8a7-4b915e39a372"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([   1,    3,    5,    8,    9,   15,   17,   21,   23,   24,   25,\n",
              "          37,   38,   39,   40,   41,   42,   43,   44,   52,   53,   63,\n",
              "          74,   75,   77,   78,   81,   82,   84,   88,   98,  101,  108,\n",
              "         114,  115,  118,  138,  140,  188,  189,  192,  194,  196,  202,\n",
              "         204,  208,  209,  210,  221,  223,  227,  237,  238,  240,  244,\n",
              "         254,  287,  305,  319,  323,  328,  361,  369,  381,  384,  397,\n",
              "         403,  417,  454,  475,  495,  507,  575,  577,  584,  585,  591,\n",
              "         607,  617,  626,  629,  641,  657,  659,  682,  683,  689,  691,\n",
              "         692,  706,  716,  725,  728,  730,  748,  755,  757,  760,  761,\n",
              "         768,  773,  777,  789,  805,  816,  823,  833,  844,  854,  858,\n",
              "         893,  918,  944, 1063, 1064, 1065, 1108, 1113, 1134, 1151, 1177,\n",
              "        1178, 1196, 1199, 1201, 1202, 1203, 1205, 1208, 1209, 1210, 1213,\n",
              "        1217, 1220, 1223, 1227, 1228, 1230, 1231, 1232, 1233, 1235, 1237,\n",
              "        1238, 1239, 1240, 1243, 1244, 1246, 1250, 1251, 1259, 1261, 1262,\n",
              "        1266, 1268, 1269]),)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.where(att[0] > halfatt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2EY-bmqVYGH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Covid_Pred_Taxonomy.ipynb",
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1nxzD1atzba5pxRhbeCs0gNB0spnhUWJh",
      "authorship_tag": "ABX9TyPuXTY4lpQPviHmNwc/6ZDV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}